[
    {
        "id": "post_1",
        "url": "https://v.redd.it/5a7eo6vat4sd1",
        "title": "OpenAI's new Whisper Turbo model running 100% locally in your browser with Transformers.js",
        "upvotes": 943,
        "rank": 1,
        "comments": [
            {
                "id": "post_1_c1",
                "text": "Earlier today, OpenAI released a new whisper model (turbo), and now it can run locally in your browser w/ Transformers.js! I was able to achieve \\~10x RTF (real-time factor), transcribing 120 seconds of audio in \\~12 seconds, on a M3 Max. Important links:\n\n- ONNX model: [https://huggingface.co/onnx-community/whisper-large-v3-turbo](https://huggingface.co/onnx-community/whisper-large-v3-turbo)  \n- Source code: [https://github.com/xenova/whisper-web/tree/experimental-webgpu](https://github.com/xenova/whisper-web/tree/experimental-webgpu)  \n- Demo: [https://huggingface.co/spaces/webml-community/whisper-large-v3-turbo-webgpu](https://huggingface.co/spaces/webml-community/whisper-large-v3-turbo-webgpu)",
                "score": 139,
                "replies": [
                    {
                        "id": "post_1_c1_r1",
                        "text": "Is there a CPU version of this, like whisper web?",
                        "score": 30
                    },
                    {
                        "id": "post_1_c1_r2",
                        "text": "Is it possible for whisper to detect multiple voices?  like a conversation, speaker 1 and speaker 2?",
                        "score": 3
                    },
                    {
                        "id": "post_1_c1_r3",
                        "text": "Is it just acting as a Middleware and hitting OpenAI servers for actual inference?",
                        "score": 9,
                        "replies": [
                            {
                                "id": "post_1_c1_r3_r1",
                                "text": "I read the code. It's using transformers.js and webgpu. So locally on the browser",
                                "score": 96,
                                "replies": [
                                    {
                                        "id": "post_1_c1_r3_r1_r1",
                                        "text": "I don't get it. How does it load a 800mb file and run it on the browser itself? Where does the model get stored? I tried it and it is fast. Doesn't feel like there was a download too.",
                                        "score": 33,
                                        "replies": [
                                            {
                                                "id": "post_1_c1_r3_r1_r1_r1",
                                                "text": "It does take a while to download for the first time. The model files are then stored in the browser's cache storage",
                                                "score": 41
                                            },
                                            {
                                                "id": "post_1_c1_r3_r1_r1_r2",
                                                "text": "[This](https://huggingface.co/onnx-community/whisper-large-v3-turbo/blob/main/onnx/decoder_model_merged_q4.onnx) is the model used. It's 300MB. With 100MBit/s it's 30 seconds, with GBit it is only 3 seconds. For some weird reason, in-browser it downloads really slow for me...\n\nDownload only starts after you click \"Transcribe Audio\".\n\n*edit* Closing Dev-tools makes download go fast.",
                                                "score": 5
                                            }
                                        ]
                                    },
                                    {
                                        "id": "post_1_c1_r3_r1_r2",
                                        "text": "Thanks for doing the due diligence that some of us can't!",
                                        "score": 13
                                    }
                                ]
                            },
                            {
                                "id": "post_1_c1_r3_r2",
                                "text": "no, that's why it only runs on Chromium browsers",
                                "score": 4
                            }
                        ]
                    }
                ]
            },
            {
                "id": "post_1_c2",
                "text": "Has anything changed with the accuracy or just speed? Having some trouble with languages other than English",
                "score": 45,
                "replies": [
                    {
                        "id": "post_1_c2_r1",
                        "text": "â€œWhisper large-v3-turbo is a distilled version of Whisper large-v3. In other words, itâ€™s the exact same model, except that the number of decoding layers have reduced from 32 to 4. As a result, the model is way faster, at the expense of a minor quality degradation.â€\n\nFrom the huggingface model card",
                        "score": 80,
                        "replies": [
                            {
                                "id": "post_1_c2_r1_r1",
                                "text": ">  decoding layers have reduced from 32 to 4\n\n> minor quality degradation\n\nwth\n\nIs there something special about STT models that makes this kind of technique so efficient?",
                                "score": 20,
                                "replies": [
                                    {
                                        "id": "post_1_c2_r1_r1_r1",
                                        "text": "You don't need many decoding layers in a STT model because the audio is already telling you what the next word will be. Nobody in the STT community uses that many layers in the decoder and it was a surprise that whisper did so when it was released. This is just openai realizing their mistake.",
                                        "score": 32,
                                        "replies": [
                                            {
                                                "id": "post_1_c2_r1_r1_r1_r1",
                                                "text": "For what it's worth, there's still accuracy degradation in the transcripts compared to the bigger model so it's really a mistake, just different goals.",
                                                "score": 13
                                            }
                                        ]
                                    },
                                    {
                                        "id": "post_1_c2_r1_r1_r2",
                                        "text": "Idk. From 1.5gb to 800mb, while becoming 8x faster with minimal quality lossâ€¦ it doesnâ€™t make sense to me. Maybe the models are just really poorly optimized?",
                                        "score": 4
                                    }
                                ]
                            }
                        ]
                    }
                ]
            },
            {
                "id": "post_1_c3",
                "text": "here is a realtime version [https://huggingface.co/spaces/kirill578/realtime-whisper-v3-turbo-webgpu](https://huggingface.co/spaces/kirill578/realtime-whisper-v3-turbo-webgpu)",
                "score": 15
            },
            {
                "id": "post_1_c4",
                "text": "Xenova, your work is incredible! Can't wait till SLMs get better.",
                "score": 14
            },
            {
                "id": "post_1_c5",
                "text": "if it's 100% localy, can it work offline?",
                "score": 21,
                "replies": [
                    {
                        "id": "post_1_c5_r1",
                        "text": "Do you mean the new whisper model? It works with whisper.cpp by ggerganov:\n\n\n> git clone https://github.com/ggerganov/whisper.cpp\n\n\n> make\n\n\n> ./main -m ggml-large-v3-turbo-q5_0.bin -f audio.wav\n\n\nAs you can see you need to point -m to where you downloaded the model and -f to the audio that you want to transcribe.\n\n\nThe model is available here: https://huggingface.co/ggerganov/whisper.cpp/tree/main",
                        "score": 33
                    }
                ]
            }
        ]
    },
    {
        "id": "post_2",
        "url": "https://www.reddit.com/r/LocalLLaMA/comments/1frjzl7/openai_plans_to_slowly_raise_prices_to_44_per/",
        "title": "OpenAI plans to slowly raise prices to $44 per month ($528 per year)",
        "upvotes": 785,
        "rank": 2,
        "comments": [
            {
                "id": "post_2_c1",
                "text": "I don't care, because I only use what I can run locally.\n\nProprietary services like ChatGPT can switch models, raise prices, suffer from outages, or even discontinue, but what's running on my own hardware is mine forever.  It will change when I decide it changes.",
                "score": 267,
                "replies": [
                    {
                        "id": "post_2_c1_r1",
                        "text": "Attaboy! Take that model by the horns!",
                        "score": 43
                    },
                    {
                        "id": "post_2_c1_r2",
                        "text": "This is the way",
                        "score": 7
                    },
                    {
                        "id": "post_2_c1_r3",
                        "text": "Do you have remote access to your local llama?",
                        "score": 7,
                        "replies": [
                            {
                                "id": "post_2_c1_r3_r1",
                                "text": "That's easy enough to configure in most cases. Worst case, you can set up a remote desktop. As long as you secure it (run it on a non-standard port, run it through Tailscale and/or use key files) it's not really an issue.",
                                "score": 9,
                                "replies": [
                                    {
                                        "id": "post_2_c1_r3_r1_r1",
                                        "text": "That's so cool",
                                        "score": 3
                                    }
                                ]
                            },
                            {
                                "id": "post_2_c1_r3_r2",
                                "text": "Of course I do.",
                                "score": 3
                            },
                            {
                                "id": "post_2_c1_r3_r3",
                                "text": "Yes I do",
                                "score": 4
                            }
                        ]
                    },
                    {
                        "id": "post_2_c1_r4",
                        "text": "Can I ask what hardware youâ€™re running it on?",
                        "score": 5,
                        "replies": [
                            {
                                "id": "post_2_c1_r4_r1",
                                "text": "Mostly I run it on a dual E5-2660v3 with 256GB of RAM and an AMD MI60 GPU with 32GB of VRAM.  Models which fit in VRAM run quite quickly, but the large system RAM means I can also use larger models and infer on CPU (which is slow as balls, but works).\n\nSometimes I run them on my laptop, a Lenovo P73 with i7-9750H and 32GB of RAM.  That lacks a useful GPU, but CPU inference again works fine (if slowly).\n\nllama.cpp gives me the flexibility of running models on GPU, on CPU, or a combination of the two (inferring on GPU for however many layers fit in VRAM, and inferring on CPU for layers which spill over into main memory).",
                                "score": 4
                            }
                        ]
                    }
                ]
            },
            {
                "id": "post_2_c2",
                "text": "This will increase the incentive to go local and drive more innovation. It also might save the planet.",
                "score": 488,
                "replies": [
                    {
                        "id": "post_2_c2_r1",
                        "text": "OpenAI also has a lot of competition.  They will eventually need the revenue to stay afloat.\n\nMistral and Claude each offer highly competitive cloud hosted models that cannot be hosted at home easily.",
                        "score": 145,
                        "replies": [
                            {
                                "id": "post_2_c2_r1_r1",
                                "text": "You also have to take into consideration that they just announced that they're going to a for-profit model so this isn't just about staying afloat, it's about increasing profits",
                                "score": 83,
                                "replies": [
                                    {
                                        "id": "post_2_c2_r1_r1_r1",
                                        "text": "They are losing 5B a year and expect to spend even more next year.\n\nThey don't have profits to increase, they are still very much trying to stay afloat.",
                                        "score": 82,
                                        "replies": [
                                            {
                                                "id": "post_2_c2_r1_r1_r1_r1",
                                                "text": "I'll love to see them die. I don't usually have a problem with corporations, but all they did was hide behind their \"non-profit\" \"public good\" image, when all Sam wanted was to mint as much money as he can for himself. I'll love to see his face when that money evaporates in front of his eyes.",
                                                "score": 58,
                                                "replies": [
                                                    {
                                                        "id": "post_2_c2_r1_r1_r1_r1_r1",
                                                        "text": "Sam is such a tool.",
                                                        "score": 30
                                                    }
                                                ]
                                            },
                                            {
                                                "id": "post_2_c2_r1_r1_r1_r2",
                                                "text": "Maybe they don't deserve to. It could just be a poor business plan",
                                                "score": 23,
                                                "replies": [
                                                    {
                                                        "id": "post_2_c2_r1_r1_r1_r2_r1",
                                                        "text": "Well, yeah. Training models is a pretty shit business model as nobody has found anything useful enough they can do that people/businesses are willing to pay enough for to make it worth it.\n\nThe whole business model is built on the idea that at some point they will actually make something worth paying for.",
                                                        "score": 21,
                                                        "replies": [
                                                            {
                                                                "id": "post_2_c2_r1_r1_r1_r2_r1_r1",
                                                                "text": "Part of the disconnect is caused by business people not understanding the technology.",
                                                                "score": 10
                                                            },
                                                            {
                                                                "id": "post_2_c2_r1_r1_r1_r2_r1_r2",
                                                                "text": "Oh there is one thingâ€¦",
                                                                "score": 3
                                                            },
                                                            {
                                                                "id": "post_2_c2_r1_r1_r1_r2_r1_r3",
                                                                "text": "Tbh I'm really happy paying for Claude right now, but I see your point because they think they can turn that into a business that costs double.",
                                                                "score": 3
                                                            }
                                                        ]
                                                    }
                                                ]
                                            }
                                        ]
                                    }
                                ]
                            },
                            {
                                "id": "post_2_c2_r1_r2",
                                "text": "Gemini is quite good too.",
                                "score": 19,
                                "replies": [
                                    {
                                        "id": "post_2_c2_r1_r2_r1",
                                        "text": "This is probably google's advantage here. They can burn 5 billion USD per year and it would not affect their bottom line much. They also own tge hardware software and data centers so the money never leaves the company anyway.",
                                        "score": 31,
                                        "replies": [
                                            {
                                                "id": "post_2_c2_r1_r2_r1_r1",
                                                "text": "And my understanding is their hardware is way more efficient. So, they can spend just as much compute per user and lose way less money, or even make money.",
                                                "score": 15,
                                                "replies": [
                                                    {
                                                        "id": "post_2_c2_r1_r2_r1_r1_r1",
                                                        "text": "Exactly. Googleâ€™s TPU are much more efficient to run AI, both training and interference. In fact Apple use that to train their AI",
                                                        "score": 12,
                                                        "replies": [
                                                            {
                                                                "id": "post_2_c2_r1_r2_r1_r1_r1_r1",
                                                                "text": "Not only that, Google has a treasure trove of data they've collected over the last 2 decades across all Google products that they now \"own\" for free, already cataloged, categorized, etc. Of all the players in the AI market they are best positioned by a long shot. They already have all the building blocks, they just need to use them.",
                                                                "score": 7,
                                                                "replies": [
                                                                    {
                                                                        "id": "post_2_c2_r1_r2_r1_r1_r1_r1_r1",
                                                                        "text": "Their execs need to get their shit together and open source model like what Facebook did. Imagine how good itâ€™ll be",
                                                                        "score": 6,
                                                                        "replies": [
                                                                            {
                                                                                "id": "post_2_c2_r1_r2_r1_r1_r1_r1_r1_r1",
                                                                                "text": "Iâ€™m confused as to how that would be best utilizing their superior position. Releasing an open source model wouldnâ€™t be especially profitable for them. Good for us, sure, them, not so much.",
                                                                                "score": 6
                                                                            },
                                                                            {
                                                                                "id": "post_2_c2_r1_r2_r1_r1_r1_r1_r1_r2",
                                                                                "text": "They already did with Gemma-2 27B",
                                                                                "score": 3
                                                                            }
                                                                        ]
                                                                    }
                                                                ]
                                                            }
                                                        ]
                                                    }
                                                ]
                                            }
                                        ]
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "id": "post_2_c2_r2",
                        "text": "I don't know what you mean by \"save the planet.\" Running an AI locally requires just as much electricity as running it in the cloud. Possibly more, since running it in the cloud allows for efficiencies of scale to come into play.",
                        "score": 54,
                        "replies": [
                            {
                                "id": "post_2_c2_r2_r1",
                                "text": "more incentives to finetune smaller models than throwing GPT-4 full at the problem and be done with it",
                                "score": 14,
                                "replies": [
                                    {
                                        "id": "post_2_c2_r2_r1_r1",
                                        "text": "Thank you, this was the point.",
                                        "score": 6
                                    },
                                    {
                                        "id": "post_2_c2_r2_r1_r2",
                                        "text": "OpenAI has incentive to make their energy usage as efficient as possible too, though.",
                                        "score": 6
                                    }
                                ]
                            },
                            {
                                "id": "post_2_c2_r2_r2",
                                "text": "It's definitely less efficient to run a local model.",
                                "score": 45,
                                "replies": [
                                    {
                                        "id": "post_2_c2_r2_r2_r1",
                                        "text": "Not in all cases.  \n\nI.e. if you use electricity for heating, your local model could be running on free electricity.",
                                        "score": 6
                                    },
                                    {
                                        "id": "post_2_c2_r2_r2_r2",
                                        "text": "Depends on how big it is and how it meets the users needs.",
                                        "score": 5,
                                        "replies": [
                                            {
                                                "id": "post_2_c2_r2_r2_r2_r1",
                                                "text": "\"How it meets the users needs\" well unless the user needs to batch, it's going to be more power efficient to use lower power data center grade hardware with increased batch size",
                                                "score": 6
                                            }
                                        ]
                                    }
                                ]
                            },
                            {
                                "id": "post_2_c2_r2_r3",
                                "text": "Also depends on where the majority of the electricity comes from for each.\n\nPeople in Quebec or British Columbia would largely be powering their inference with hydroelectricity. 95+%, and 90+% respectively. Hard to get much greener than that.\n\nWhile OpenAI is largely on the Azure platform, which puts a lot of their data centres near nuclear power plants and renewables, they're still pulling electricity from grids that have significant amounts of fossil fuel plants.",
                                "score": 11,
                                "replies": [
                                    {
                                        "id": "post_2_c2_r2_r3_r1",
                                        "text": "This sounds like an argument in favor of the big data centers to me, since they can be located near power sources like those more easily. Distributed demand via local models will draw power from a much more diverse set of sources.",
                                        "score": 6
                                    }
                                ]
                            },
                            {
                                "id": "post_2_c2_r2_r4",
                                "text": "Not true at all, you can use a Mac Studio idling at 15w and around 160w  max using 70 or 140B models at a perfectly usable speed for one person local use",
                                "score": 3
                            },
                            {
                                "id": "post_2_c2_r2_r5",
                                "text": "Less people will run AI over all",
                                "score": 6,
                                "replies": [
                                    {
                                        "id": "post_2_c2_r2_r5_r1",
                                        "text": "You're assuming that demand for AI services aren't borne from genuine desire for them. If the demand arises organically then the supply to meet it will also be organic.",
                                        "score": 5
                                    },
                                    {
                                        "id": "post_2_c2_r2_r5_r2",
                                        "text": "Good logic redditor, yeah people will simply stop using AI while AI gets better and more intelligent every year, increasing the productivity of AI users vs. non-users\n\nSure ðŸ‘",
                                        "score": 9
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "id": "post_2_c2_r3",
                        "text": "Also for how cheap the api is if you're not using massive amounts of context constantly, I won't be surprised if people just switch to a different front end with an API key",
                        "score": 7
                    }
                ]
            },
            {
                "id": "post_2_c3",
                "text": "Nice. I'm out.",
                "score": 191,
                "replies": [
                    {
                        "id": "post_2_c3_r1",
                        "text": "Yep, same. what did we expect.",
                        "score": 37,
                        "replies": [
                            {
                                "id": "post_2_c3_r1_r1",
                                "text": "If OpenAI loses half of their customers from this they still benefit since their profits stay the same and their server costs go down since less people are subscribed.",
                                "score": 8,
                                "replies": [
                                    {
                                        "id": "post_2_c3_r1_r1_r1",
                                        "text": "Not really, training cost is still a huge burden. And the more users you have in the platform the more they can distribute those costs per user.",
                                        "score": 4,
                                        "replies": [
                                            {
                                                "id": "post_2_c3_r1_r1_r1_r1",
                                                "text": "Training costs are a fixed amount that is independent of the number of users, they donâ€™t gain anything by distributing the costs over more users.",
                                                "score": 7
                                            }
                                        ]
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "id": "post_2_c3_r2",
                        "text": "Yeah, if it goes any higher Iâ€™ll immediately find something else.\nWhat I use it for is easily replaced by other models without lesser quality.",
                        "score": 24
                    },
                    {
                        "id": "post_2_c3_r3",
                        "text": "I was already gone since the quality dropped dramatically. Now Iâ€™m not coming back, ever.",
                        "score": 4
                    },
                    {
                        "id": "post_2_c3_r4",
                        "text": "Consider that they might start offering products worth $44 a month, if not more",
                        "score": 10
                    }
                ]
            },
            {
                "id": "post_2_c4",
                "text": "I unsubscribed because they went closed and started calling for regulation.  At the end of the day it's about value.  If you are going to become more productive then it will be worth it.   Many people are not going to go local LLM.  I can't even get plenty of tech folks/programmers I know to run local LLM.",
                "score": 17
            },
            {
                "id": "post_2_c5",
                "text": "Then they will have 5 million subscribers. Raise needs more features, voice is not enough.\n\nThrough API I even didn't spend $5 from be beginning of the year.",
                "score": 82,
                "replies": [
                    {
                        "id": "post_2_c5_r1",
                        "text": "Yeah, call me crazy but OpenAI will probably release more stuff in that 5  years",
                        "score": 62,
                        "replies": [
                            {
                                "id": "post_2_c5_r1_r1",
                                "text": "Yep. I'm glad we have some competition, but as of now it seems like every other company are just chasing the leader.",
                                "score": 6
                            }
                        ]
                    },
                    {
                        "id": "post_2_c5_r2",
                        "text": "I said it above but you hit in the same point: you can just switch to a comparable front end with an api key",
                        "score": 9
                    },
                    {
                        "id": "post_2_c5_r3",
                        "text": "Waitâ€¦\nSo I can use ChatGPT for cheaper than what openAI charge?",
                        "score": 4,
                        "replies": [
                            {
                                "id": "post_2_c5_r3_r1",
                                "text": "It depends on how much you use it, but in most cases, yes.",
                                "score": 6
                            },
                            {
                                "id": "post_2_c5_r3_r2",
                                "text": "The subscription includes stuff like advanced voice mode, memory and Dall-E, you won't get the same experience with API. If you just care about the chat then yes.",
                                "score": 3
                            }
                        ]
                    },
                    {
                        "id": "post_2_c5_r4",
                        "text": "They are losing $5 billion this year, they have no choice but to increase the price.\n\n[https://www.cnbc.com/2024/09/27/openai-sees-5-billion-loss-this-year-on-3point7-billion-in-revenue.html](https://www.cnbc.com/2024/09/27/openai-sees-5-billion-loss-this-year-on-3point7-billion-in-revenue.html)",
                        "score": 4
                    }
                ]
            },
            {
                "id": "post_2_c6",
                "text": "Inference costs of LLMs should fall soon after inference chips ramp up production and popularity. Gpu's aren't the best way to do inference, both price wise and speed wise.\n\nOpenAI isn't positioned well to use that due to their incredibly strong link to Microsoft. Microsoft wants llm training and inference to be expensive so that they can profit the most and will be unlikely to set up those custom llm accelerators quickly.\n\nI hope OpenAI won't be able to get an edge where they can be strongly profitable.",
                "score": 25
            },
            {
                "id": "post_2_c7",
                "text": "Good luck with that. The results between high and medium level models are already becoming marginal. I don't even find the much hyped o1 to be any better than Claude. The only thing not making the LLMs utilitarian at this point are Jensen's costly leather jackets. Once more silicon becomes available, I wouldn't be surprised if they have to actually cut the costs.",
                "score": 50,
                "replies": [
                    {
                        "id": "post_2_c7_r1",
                        "text": "OpenAI and Anthropic are losing billions of dollars. As does everyone actually developing models.\n\nEveryone is still very much looking for a way to make money on this as nobody has found it yet.\n\nSo the prices will go up once the investors start asking for return on investment pretty much across the board.",
                        "score": 35,
                        "replies": [
                            {
                                "id": "post_2_c7_r1_r1",
                                "text": "But will users see the value? If they lose users, they may have to lower prices again",
                                "score": 10,
                                "replies": [
                                    {
                                        "id": "post_2_c7_r1_r1_r1",
                                        "text": "It'll be like Salesforce where after they get firmly embedded in a business critical way that's not easily switched by swapping an API key, they'll jack up the prices.Â ",
                                        "score": 16
                                    }
                                ]
                            },
                            {
                                "id": "post_2_c7_r1_r2",
                                "text": ">OpenAI and Anthropic are losing billions of dollars. As does everyone actually developing models.\n\nSpending is very different to losing. They're paying to build very valuable models.",
                                "score": 6,
                                "replies": [
                                    {
                                        "id": "post_2_c7_r1_r2_r1",
                                        "text": ">Spending is very different to losing\n\nYes it is. Losing is when you take your revenue deduct from it your costs and you are still in negative.\n\nThings are only as valuable as somebody is willing to pay for it. \n\nThese models are potentially very valuable, but they have been having trouble actually selling it to people and businesses at price that makes it worth it.",
                                        "score": 7
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "id": "post_2_c7_r2",
                        "text": "It's not even about more silicon, it's more about using that silicon effectively, even GPU mining started manufacturing ASICs, if we don't see an ASIC LLM in 5 years I'd be really really surprised at least for the big companies hosting.",
                        "score": 3
                    }
                ]
            },
            {
                "id": "post_2_c8",
                "text": "[deleted]",
                "score": 11
            },
            {
                "id": "post_2_c9",
                "text": "Good. There will have to be cheaper alternatives. If they had dominated 20$ range, there would be no competition.",
                "score": 19
            },
            {
                "id": "post_2_c10",
                "text": "XX90 card would pay for itself",
                "score": 16,
                "replies": [
                    {
                        "id": "post_2_c10_r1",
                        "text": "But you can't run much on a single 4090 or even 3090. Best you can do is a 70B model with aggressive quantisation.\n\nNo Mistral Large 2 (123B) or Command R+ (104B) for example, unless you use normal RAM (but then you may have to wait 20-30 min or more for an answer)",
                        "score": 11,
                        "replies": [
                            {
                                "id": "post_2_c10_r1_r1",
                                "text": "Have you checked how good a 22B is these days? Also consider in 5 years we'll probably have A100s flooding the used market, not to mention better consumer cards.\n\nIt's only going to get better.",
                                "score": 18,
                                "replies": [
                                    {
                                        "id": "post_2_c10_r1_r1_r1",
                                        "text": ">Have you checked how good a 22B is these days?\n\nYep, a 22B is pretty bad to me. In my opinion and use case, even Llama 3.1 70b, Command R+ 104B and Mistral Large 2407 123b come close, but not match, GPT-4o and GPT-4o1p. \n\n22B can't even compete.\n\nCoding\\\\IT use case. Just my opinion, I don't expect everyone to agree.\n\n>Also consider in 5 years we'll probably have A100s flooding the used market\n\nYep, but they are like 20.000â‚¬ right now. It's not like paying half of that would make me able to afford them.\n\n>It's only going to get better.\n\nYes, on the local end, indeed. What we have now is better than the first GPT iterations. Still, when we'll have better local models, OpenAI and others will have much better and the gap will always be there, as long as they keep innovating.\n\nEven if they don't, they have a ton of compute to throw at it, which you don't have locally.",
                                        "score": 5,
                                        "replies": [
                                            {
                                                "id": "post_2_c10_r1_r1_r1_r1",
                                                "text": "Try Qwen2.5 72b on the system you're currently running Mistral-Large on.\n\nI haven't used the Sonnet3.5 API since",
                                                "score": 4
                                            }
                                        ]
                                    }
                                ]
                            },
                            {
                                "id": "post_2_c10_r1_r2",
                                "text": "Models that can fit well within a 3090's VRAM, and are only marginally behind GPT 4, exist and are getting more common by the day.",
                                "score": 12,
                                "replies": [
                                    {
                                        "id": "post_2_c10_r1_r2_r1",
                                        "text": "Nothing that comes close to gpt 4o fits in 24 GB of VRAM a 4090 has. You have to quant to Q3 or Q4 and dumb down the thing even further. Even with 128gb of RAM, you'll be under memory pressure to run a mistral large at full q8",
                                        "score": 5,
                                        "replies": [
                                            {
                                                "id": "post_2_c10_r1_r2_r1_r1",
                                                "text": "Gemma2 27B Q6_K_M (or 5, i forget) comes close to gpt4o and 98% of it fits in VRAM. The speed is still good even with the offloading to sys ram.\n\nThat model outperforms gpt4 in some tasks.",
                                                "score": 6
                                            }
                                        ]
                                    }
                                ]
                            }
                        ]
                    }
                ]
            },
            {
                "id": "post_2_c11",
                "text": "I get how the average person doesn't know/understand/care enough to setup their own chat with an openai key, but for other people why wouldn't you? What do you get out of chatgpt plus subscription versus just using the openai API with an open source chat interface?",
                "score": 39,
                "replies": [
                    {
                        "id": "post_2_c11_r1",
                        "text": "OpenAI's consumer product is a RAG web app. That web app happens to be powered by a really good proprietary LLM. It is a completely different product than their SaaS product, what comes out of the API.Â Â The average person wants an LLM that meets them half way. They are best served by the web app.\n\n\nOpenAI cannot win the SaaS LLM market, so they are burning money racing to lock down the consumer market.",
                        "score": 51
                    },
                    {
                        "id": "post_2_c11_r2",
                        "text": "The subscription is cheaper than API usage if you use it often. Especially if you use o1.",
                        "score": 24,
                        "replies": [
                            {
                                "id": "post_2_c11_r2_r1",
                                "text": "O1 is crazy expensive because they are double dipping. Not only did they pump up the price of the model 6x per token, but they are also charging you for the thinking tokens.\n\nIMO, if the speculation that the underlying model is the same as 4o, then the cost per token should be the same as 4o (10$/m), and the extra cost should come from the reasoning tokens. Or if they really want to charge a premium, then make it 15$ or something, but 60 is insane. The only reason they do it is because it's currently the only such product on the market (not for long though).",
                                "score": 10
                            }
                        ]
                    },
                    {
                        "id": "post_2_c11_r3",
                        "text": "I donâ€™t really want to worry about running a bill up on the api. $30 per month is fine for me for a tool I use every single day, and helps me with both personal, and in my career lol.",
                        "score": 8
                    },
                    {
                        "id": "post_2_c11_r4",
                        "text": "You know that they're going to raise the costs on the API too, right? They're giving it away at a big discount now to try and take the lead on all things related to hosted AI services.",
                        "score": 20,
                        "replies": [
                            {
                                "id": "post_2_c11_r4_r1",
                                "text": "They canâ€™t raise it too much without people leaving for Claude/Gemini.",
                                "score": 6,
                                "replies": [
                                    {
                                        "id": "post_2_c11_r4_r1_r1",
                                        "text": "These are also losing billions of dollars a year like OpenAI. They will sooner or later need to raise prices as well.\n\nGoogle might be somewhat limiting their losses by using their own chips, concentrating on efficiency and not trying to release the best, biggest model there is.\n\nBut even with that they would still be losing billions on this.",
                                        "score": 6
                                    }
                                ]
                            },
                            {
                                "id": "post_2_c11_r4_r2",
                                "text": "Even if they do, I doubt I'll ever reach a $528 bill for API calls in a year.  Also, there are other alternatives. Use Openrouter and you can choose any provider for basically any popular model.",
                                "score": 6,
                                "replies": [
                                    {
                                        "id": "post_2_c11_r4_r2_r1",
                                        "text": "Depends on how much you use it. When used for work I easily get to 5-10$ per day of API usage.",
                                        "score": 9
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "id": "post_2_c11_r5",
                        "text": "Depending on usage pattern, API could cost more than the subscription.",
                        "score": 5
                    },
                    {
                        "id": "post_2_c11_r6",
                        "text": "You get good multilingual capabilities (most open weight models don't support my language besides one that's 340B params..).  \nAlso advanced voice mode is cool.  \nBut that's about it and I guess the coding is ok, you get to use it for free at least (not sure if there's any GPT-4o level 7-12B param models for coding).",
                        "score": 3
                    },
                    {
                        "id": "post_2_c11_r7",
                        "text": "Also there are so many alternatives. I use right now Gemini 1.5 002 Pro in AIStudio and it got a huge boost too with the last upg and it is really good easy on the GPT4o level. \n\nAlso free, hit like once a rate limit last week. \n\nEnough competition so openAI can do what they like.",
                        "score": 3
                    },
                    {
                        "id": "post_2_c11_r10",
                        "text": "> What do you get out of chatgpt plus subscription versus just using the openai API with an open source chat interface?\n\nmost people just want the brand name that is the most well established as being \"the best\". OpenAI has made the most headlines by far and they dominate the leader boards. personally, i think the leader boards need to enhance their security or something because there is no fucking way that GPT models dominate all the top spots while claude sonnet is 7th place. thats crazy. either these boards are being gamed hard or they are accepting bribes.",
                        "score": 3
                    }
                ]
            },
            {
                "id": "post_2_c14",
                "text": "Rubbish lmao\n\n\nModels that will exist in just 3 years are unimaginable.Â ",
                "score": 18
            },
            {
                "id": "post_2_c15",
                "text": "I will not pay >$20 a month, immediately cancelling if that happens.",
                "score": 22,
                "replies": [
                    {
                        "id": "post_2_c15_r1",
                        "text": "I might pay the $22 a month, but not more than $25 a month",
                        "score": 11,
                        "replies": [
                            {
                                "id": "post_2_c15_r1_r1",
                                "text": "A year or two is a long time for competition to catch up. Though I guess a year or two is a long time for them to make chatgpt better",
                                "score": 3
                            }
                        ]
                    }
                ]
            },
            {
                "id": "post_2_c22",
                "text": "i don't care if it's over 5 years, honestly, by that time we'll be eons ahead of what we have now. Given how much it improves my life and work, it's well worth it.",
                "score": 12
            },
            {
                "id": "post_2_c35",
                "text": "How will I connect LocalLlama to my smartphone? Will I have as good Voice Advanced Mode as ChatGPT? Does electricity of running my own PC with LocalLlama is free?",
                "score": 20,
                "replies": [
                    {
                        "id": "post_2_c35_r1",
                        "text": "Still 40 bucks a month is 200kw/h (600 hours of 3090 at near max power, so 25 days) at 20 cents the kw/h a VPN can be very inexpensive or free.. And yeah come back in a couple of months voice won't be an issue",
                        "score": 5,
                        "replies": [
                            {
                                "id": "post_2_c35_r1_r1",
                                "text": "I worked out that is about what it would cost me to run a high-idle power AI server in my high electricity cost location. I'm cheap, so I don't want to pay $40 per month in API or electricity costs. I plan to have a basic low power AI server for basic tasks that has the ability to spin up the big one on-demand. This will reduce electricity costs to $6 per month.\n\nAdding in the capital costs, it will take 2.5 years to pay back. Having said that, for me, the benefit of local is really in the learning. I learned so much doing this and I find that valuable too.",
                                "score": 3
                            }
                        ]
                    },
                    {
                        "id": "post_2_c35_r2",
                        "text": "You shouldn't be downvoted because we are obvs local llm community. These are all valid points local has to contend with. Electricity in particular. Need to figure out how much I'm spending a month to run my own system. Not that I will stop, but just to get a clearer picture of costs and value.",
                        "score": 13
                    },
                    {
                        "id": "post_2_c35_r4",
                        "text": "Electricity costs of running local are usually negligible compared to API or subscription costs, but that depends where you live.\n\nAs for how you connect local models to your smartphone, right now the answer is build your own implementation or look up what other people have done for that. This stuff is cutting edge and open source at its best isn't usually known for easy pre-packaged solutions for non-technical people (I wish it wasn't that way, but it is, and I hope it gets better.)\n\nWill you have as good voice mode as chatGPT? If past open source progress is any indication, yes. \"When\" is more subjective but my take is \"soon\".",
                        "score": 4
                    }
                ]
            }
        ]
    },
    {
        "id": "post_3",
        "url": "https://i.redd.it/hoojuqlpzdsd1.png",
        "title": "Those two guys were once friends and wanted AI to be free for everyone",
        "upvotes": 710,
        "rank": 3,
        "comments": [
            {
                "id": "post_3_c1",
                "text": "AI being \"free\" for everyone wasn't even in the original charter. It was a vague \"AGI for all\" idea but mostly it was to not let Google completely monopolise AI research.\n\nIf you look at Musk's old interviews at the time talent and resources were concentrated in Google to the tune of 90+%, which is evident from the fact that every notable scientist at OAI even today is ex-Google/Deepmind.",
                "score": 432,
                "replies": [
                    {
                        "id": "post_3_c1_r1",
                        "text": "I knew someone from deep mind, before gpt. She had signed so many NDAs she couldn't really speak about her work. Google really messed up",
                        "score": 115,
                        "replies": [
                            {
                                "id": "post_3_c1_r1_r1",
                                "text": "It is amazing how much talent deep mind took in and did nothing with because it would hurt search ads.\n\nI'm reminded of Bell Labs killing magnetic storage because they were afraid people would stop using telephones if they could be recorded.",
                                "score": 47,
                                "replies": [
                                    {
                                        "id": "post_3_c1_r1_r1_r1",
                                        "text": "Kodak had 10 year lead on digital cameras, instead of innovating they buried it.",
                                        "score": 10
                                    },
                                    {
                                        "id": "post_3_c1_r1_r1_r2",
                                        "text": "crazy if you think about it",
                                        "score": 8,
                                        "replies": [
                                            {
                                                "id": "post_3_c1_r1_r1_r2_r1",
                                                "text": "Its the result of an out of touch managment structure that just wants to keep the bonuses and early retirement train rolling with what they already know",
                                                "score": 5
                                            }
                                        ]
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "id": "post_3_c1_r2",
                        "text": "Google fucked up big time",
                        "score": 85,
                        "replies": [
                            {
                                "id": "post_3_c1_r2_r1",
                                "text": "They literally invented the transformer and yet failed to make a decent LLM before others lol",
                                "score": 88,
                                "replies": [
                                    {
                                        "id": "post_3_c1_r2_r1_r1",
                                        "text": "they invented big data, and then did nothing, so a pervert making a database of chicks in his college became the social media trillionaire\n\nthey invented borg, and then did nothing, so a second-hand book seller became the cloud computing trillionaire",
                                        "score": 11
                                    },
                                    {
                                        "id": "post_3_c1_r2_r1_r2",
                                        "text": "this is what blows my mind. gemini is so ass.",
                                        "score": 48,
                                        "replies": [
                                            {
                                                "id": "post_3_c1_r2_r1_r2_r1",
                                                "text": "Sure but it has a 1.5M context window which is an order of mag breakthrough on one of the most critical limitations of LLMs right now. i.e. Google continue to innovate in the fundamentals while others keep riffing on the fundamental breakthroughs coming out of Google. I think what Google sucks at is productizing their breakthroughs. They have the math, they just don't do well interacting with humans.",
                                                "score": 75,
                                                "replies": [
                                                    {
                                                        "id": "post_3_c1_r2_r1_r2_r1_r1",
                                                        "text": "This. AI charlatans and/or people outside of the field can not appreciate enough this comment. Google could win the LLM race just because of this breakthrough.",
                                                        "score": 43
                                                    },
                                                    {
                                                        "id": "post_3_c1_r2_r1_r2_r1_r2",
                                                        "text": "Context is king!",
                                                        "score": 13
                                                    },
                                                    {
                                                        "id": "post_3_c1_r2_r1_r2_r1_r3",
                                                        "text": "Oh come on, it's not just that. Gemini lags ChatGPT substantially.",
                                                        "score": 3
                                                    }
                                                ]
                                            }
                                        ]
                                    }
                                ]
                            },
                            {
                                "id": "post_3_c1_r2_r2",
                                "text": "Did they? The AGI race isn't over yet. OpenAi is losing a ton of money just to keep their computers spinning, and they announce and release everything they are working on. I think Google has a lot of things they are working on but not releasing yet.",
                                "score": 18
                            },
                            {
                                "id": "post_3_c1_r2_r3",
                                "text": "Yup",
                                "score": 9
                            }
                        ]
                    },
                    {
                        "id": "post_3_c1_r3",
                        "text": "> mostly it was to not let Google completely monopolise AI research.\n\nand for this, I thank them. Remember, we had YEARS of \"look at this neat thing we have behind the red rope\". If not for OAI, we'd still be marveling at all the cool stuff behind the rope that Google refuses to share.",
                        "score": 9
                    },
                    {
                        "id": "post_3_c1_r4",
                        "text": "Google had an opportunity, they messed up by keeping everything 'top secret' and not letting the community pitch in. Now they're playing catch up",
                        "score": 35,
                        "replies": [
                            {
                                "id": "post_3_c1_r4_r1",
                                "text": "Except for that transformer paper, â€œAttention Is All You Needâ€.",
                                "score": 38,
                                "replies": [
                                    {
                                        "id": "post_3_c1_r4_r1_r1",
                                        "text": "Should have considered that when they were ignoring competition.",
                                        "score": 3,
                                        "replies": [
                                            {
                                                "id": "post_3_c1_r4_r1_r1_r1",
                                                "text": "Meta is doing good. Very surprisingly. Though they're playing catch up too but are leveraging the communities knowledge and interest. I'm sure they're going back track though at some point. But for the moment, I'm enjoying their contribution",
                                                "score": 9,
                                                "replies": [
                                                    {
                                                        "id": "post_3_c1_r4_r1_r1_r1_r1",
                                                        "text": "Meta is filling a completely different niche IMO. If anybody is developing \"AI for all\" it's Meta, not Google or OAI.  He may not have meant it as such, but this has been one hell of a redemption arc for Mark.",
                                                        "score": 16
                                                    }
                                                ]
                                            }
                                        ]
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "id": "post_3_c1_r5",
                        "text": "Such a disappointment considering Google created the transformer, and even created a chat-tuned model. I try to imagine what the world would look like if that crazy ex-Googleâ€™s didnâ€™t â€œwhistle blowâ€ that google made a sentient AI. Google may never have been too scared to show off their chat bot sooner, but also OpenAI may never have tried to make their own.",
                        "score": 11,
                        "replies": [
                            {
                                "id": "post_3_c1_r5_r1",
                                "text": "Thinking back, what was this whole \"sentient AI\" about, really? Anyone who has at least *basic* insight into how an LLM works, knows that it is anything but conscious. It can't be possible that a developer, who even probably have a deeper knowledge of the technology, really seriously believed that it had a consciousness.\n\nWhat was the true reason to claim this?",
                                "score": 3,
                                "replies": [
                                    {
                                        "id": "post_3_c1_r5_r1_r1",
                                        "text": "I seem to recall (but canâ€™t find a source) that he didnâ€™t work directly as a developer in that space. He is also a â€œmystic priestâ€ who wanted to work on AI because of spiritual beliefs, so it seems like he couldâ€™ve been looking for something to confirm his preconceived beliefs",
                                        "score": 6
                                    }
                                ]
                            },
                            {
                                "id": "post_3_c1_r5_r2",
                                "text": "Decoder only transformers was a novel research direction by OpenAI. Google was doing Bert, T5, and and the entire muppet show when Open AI was releasing GPT2. At the time GPT2 didn't seem like it would be the architecture to take over. I think everyone outside of OpenAI was a bit surprised by ChatGPT.",
                                "score": 9
                            },
                            {
                                "id": "post_3_c1_r5_r3",
                                "text": "I think you might have your timeline mixed up a bit. The whistleblower thing happened in April 2023. ChatGPT had already been a thing for several months by that point.",
                                "score": 5,
                                "replies": [
                                    {
                                        "id": "post_3_c1_r5_r3_r1",
                                        "text": "> The whistleblower thing happened in April 2023\n\n[Here](https://www.theguardian.com/technology/2022/jul/23/google-fires-software-engineer-who-claims-ai-chatbot-is-sentient) is a link to the story from July 2022. The dates are close enough that the whistleblowing clearly wasn't what stopped Google from releasing something before OpenAI, but I'm pretty sure it predated ChatGPT.",
                                        "score": 11
                                    }
                                ]
                            }
                        ]
                    }
                ]
            },
            {
                "id": "post_3_c2",
                "text": "It really irritates me when someone put their feet on the sofa with shoes on.",
                "score": 69,
                "replies": [
                    {
                        "id": "post_3_c2_r1",
                        "text": "That was the first thing that I noticed. Get yer damn feet off the chair, you heathen. Casual disrespect for other folks' stuff.",
                        "score": 9
                    }
                ]
            },
            {
                "id": "post_3_c3",
                "text": "Sama/closed.ai ðŸ¤®ðŸ™…\n\nMark/meta.ai  ðŸ¦™ðŸ‘",
                "score": 171,
                "replies": [
                    {
                        "id": "post_3_c3_r1",
                        "text": "never in a million years would I have thought to be on meta's side when it came to anything -- but I really appreciate their contributions to open source stuff in the LLM space",
                        "score": 115,
                        "replies": [
                            {
                                "id": "post_3_c3_r1_r1",
                                "text": "Exactly my thoughts when 3.1 and 3.2 got released after o1 announcement.",
                                "score": 20,
                                "replies": [
                                    {
                                        "id": "post_3_c3_r1_r1_r1",
                                        "text": "I'm excited to see their open source voice model that I believe is coming.",
                                        "score": 14,
                                        "replies": [
                                            {
                                                "id": "post_3_c3_r1_r1_r1_r1",
                                                "text": "The thing is...\nI still have no idea why Zuckerberg is helping us, and that makes me very nervous.",
                                                "score": 20,
                                                "replies": [
                                                    {
                                                        "id": "post_3_c3_r1_r1_r1_r1_r1",
                                                        "text": "Zuck has said in multiple interviews why\n\nBasically itâ€™s unlikely that Meta can out-perform the rest of the world forever, so theyâ€™d rather cultivate the ecosystem around their tools so they can use whatever open source stuff comes around in the future. Plus they have actual use cases for AI, so they want to ensure they have good and audited models for their own products. \n\nTheyâ€™ve done this before, eg Open Compute for data center hardware. \n\nI also suspect ego of Zuck plays a role, heâ€™s always been a dev at heart so I think he wants to make tools and products for devs for his own clout and personal motives.",
                                                        "score": 38
                                                    },
                                                    {
                                                        "id": "post_3_c3_r1_r1_r1_r1_r2",
                                                        "text": "It's the same play that made Facebook the social media winner.\n\nStep 1: Offer the product for free. This decapitates any company that needs to show P&L.\n\nStep 2: Use a gigantic capital and scale advantage to outlast and buy up competitors. \n\nStep 3: What remains is yours.",
                                                        "score": 13
                                                    },
                                                    {
                                                        "id": "post_3_c3_r1_r1_r1_r1_r3",
                                                        "text": "The cynical view is that it's not FOSS. it's open source but has the most restrictive enterprise license. Even restricting any products created with it to have the model name in it.\n\nThis isn't a bad thing, it's just not completely selfless as there is a monetization model that goes with it",
                                                        "score": 7
                                                    }
                                                ]
                                            }
                                        ]
                                    },
                                    {
                                        "id": "post_3_c3_r1_r1_r2",
                                        "text": "3.2 is only 2B?",
                                        "score": 3
                                    }
                                ]
                            },
                            {
                                "id": "post_3_c3_r1_r2",
                                "text": "The burning question is why they're doing it",
                                "score": 3,
                                "replies": [
                                    {
                                        "id": "post_3_c3_r1_r2_r1",
                                        "text": "Sometimes you win by playing by the rules, sometimes you win by burning the rulebook and letting everyone else fight amongst themselves.\n\nMeta chooses the latter.",
                                        "score": 6
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "id": "post_3_c3_r2",
                        "text": "It's been a wild few years, eh?",
                        "score": 17
                    }
                ]
            },
            {
                "id": "post_3_c4",
                "text": "People are this gullible, huh.",
                "score": 70,
                "replies": [
                    {
                        "id": "post_3_c4_r1",
                        "text": "Simping for billionaires will never die, it seems",
                        "score": 12
                    },
                    {
                        "id": "post_3_c4_r2",
                        "text": "People really be simping for Mark Zuckerberg here too.",
                        "score": 12
                    },
                    {
                        "id": "post_3_c4_r3",
                        "text": "PR strategies like this work, otherwise corporations wouldn't engage in them. It's crazy to me too, but there are people out there who will buy the \"we were trying to help humanity and just fell into a pile of money\" line.",
                        "score": 6
                    },
                    {
                        "id": "post_3_c4_r4",
                        "text": "Stunning, isn't it. I genuinely can't fathom thinking that the world works like that.",
                        "score": 3
                    }
                ]
            },
            {
                "id": "post_3_c5",
                "text": "1- Sociopaths don't have friends, they are incapable of it.\n\n2- They don't care about you, me or anyone else.",
                "score": 52,
                "replies": [
                    {
                        "id": "post_3_c5_r1",
                        "text": "I'd say Musk is a narcissist. He might not care what an individual thinks, but he absolutely needs validation from society to survive. Just look at how much he tweets. The man needs more attention than a girl in high school. He craves validation way, way too much, which makes him feel narcissistic.",
                        "score": 14
                    }
                ]
            },
            {
                "id": "post_3_c6",
                "text": "Almost like people shouldn't be idolized because every single person is deeply flawed.... almost like we shouldn't let people accumulate such vast quantities of wealth because they will force their flaws upon you, and strip you of your autonomy.",
                "score": 21
            },
            {
                "id": "post_3_c7",
                "text": "lol neither of them is capable of forming meaningful relationships.\n\n\nStop idolizing rich people as if they are visionaries.",
                "score": 86
            },
            {
                "id": "post_3_c8",
                "text": "Just my opinion,  \n  \nThey were never friends, just 2 people that really know how to use and exploit others.\n\nThey never wanted AI to be free, they wanted the wealth & power that came from AI and their plan was to play nice and see who could grab the pie.   Elon let go of the pie because he didn't think it was going to happen, Sama got the pie by not only grabbing unto it but figuring out how to make everyone that was holding a little piece from the get go  to give up and leave.",
                "score": 30,
                "replies": [
                    {
                        "id": "post_3_c8_r1",
                        "text": ">2 people that really know how to use and exploit others\n\nYou just described pretty much everyone running a large company.",
                        "score": 3
                    }
                ]
            },
            {
                "id": "post_3_c9",
                "text": ">Those two guys were once friends and wanted AI to be free for everyone\n\nI've heard of more honest prostitutes.",
                "score": 31
            },
            {
                "id": "post_3_c16",
                "text": "People should buy PCs with as much as RAM they could get, same for VRAM on graphics cards\n\nI have 128GB RAM and I enjoy LocalLLaMA\n\nDon't become the slaves of those tech companies",
                "score": 16,
                "replies": [
                    {
                        "id": "post_3_c16_r1",
                        "text": "Crying with my shitty 64gb RAM )",
                        "score": 8,
                        "replies": [
                            {
                                "id": "post_3_c16_r1_r1",
                                "text": "That time you email your LLM like an offshore colleague and don't expect a reply until the following day...",
                                "score": 10
                            }
                        ]
                    },
                    {
                        "id": "post_3_c16_r2",
                        "text": "The only thing worse than being a slave to tech companies is being a slave to some dumb ideologue that makes you use a dumber model with horrible latency just to have bragging rights with fellow irrational ideologue losers.\n\nI'm not even going to comment on the irony of using models \"donated by tech companies to advance their own strategic interest\" on a rig built by another big tech company and claiming that they aren't slave to tech companies.",
                        "score": 3,
                        "replies": [
                            {
                                "id": "post_3_c16_r2_r1",
                                "text": "I just wanted AI that can say \"fuck\".",
                                "score": 6
                            },
                            {
                                "id": "post_3_c16_r2_r2",
                                "text": "But at least the model responds to 'Yass queen' instead of shutting you down.",
                                "score": 6
                            }
                        ]
                    }
                ]
            },
            {
                "id": "post_3_c20",
                "text": "https://preview.redd.it/qjny0lju0esd1.jpeg?width=2000&format=pjpg&auto=webp&s=132f6d02339309c442bfdb71053d29020699d77f",
                "score": 12
            },
            {
                "id": "post_3_c21",
                "text": "Off course they wanted it...",
                "score": 10
            }
        ]
    },
    {
        "id": "post_4",
        "url": "https://videocardz.com/newz/nvidia-geforce-rtx-5090-and-rtx-5080-specs-leaked",
        "title": "RTX 5090 will feature 32GB of GDDR7 (1568 GB/s) memory",
        "upvotes": 707,
        "rank": 4,
        "comments": [
            {
                "id": "post_4_c1",
                "text": "And apparently the 5080 will still have 16 GB. Of course.",
                "score": 401,
                "replies": [
                    {
                        "id": "post_4_c1_r1",
                        "text": "lol at the 3090 staying the 2nd best card forever",
                        "score": 116,
                        "replies": [
                            {
                                "id": "post_4_c1_r1_r1",
                                "text": "well 3rd best card.",
                                "score": 23
                            }
                        ]
                    },
                    {
                        "id": "post_4_c1_r2",
                        "text": "turd",
                        "score": 73
                    },
                    {
                        "id": "post_4_c1_r3",
                        "text": "Hope they \"unlaunch\" it like they did with the 12 GB 4080.",
                        "score": 17,
                        "replies": [
                            {
                                "id": "post_4_c1_r3_r1",
                                "text": "Can't believe we forgot about that. What a pathetic waste that would have been, an 80 series card with half the VRAM of the 90 series one to make people feel pressured to buy the extremely overpriced one? Oh, wait...",
                                "score": 19
                            }
                        ]
                    },
                    {
                        "id": "post_4_c1_r4",
                        "text": "16gb card pulling 400w lol. Guess I'm going to be buying a 4070 TS real soon.",
                        "score": 32,
                        "replies": [
                            {
                                "id": "post_4_c1_r4_r1",
                                "text": "4080 Super if you can afford it",
                                "score": 4
                            }
                        ]
                    },
                    {
                        "id": "post_4_c1_r5",
                        "text": "Leather jacket man fails again.",
                        "score": 11
                    },
                    {
                        "id": "post_4_c1_r6",
                        "text": "I had some Nvidiot start lecturing me about how not all VRAM is equal. Some people defend this nonsense.",
                        "score": 3
                    }
                ]
            },
            {
                "id": "post_4_c2",
                "text": "and will cost like $3500 ðŸ˜­",
                "score": 279,
                "replies": [
                    {
                        "id": "post_4_c2_r1",
                        "text": "It will cost 5090$ it's in the name",
                        "score": 297,
                        "replies": [
                            {
                                "id": "post_4_c2_r1_r1",
                                "text": "In that case, more like $5099.99",
                                "score": 10
                            }
                        ]
                    },
                    {
                        "id": "post_4_c2_r2",
                        "text": "In before the â€œshould I get 2x 4090â€™s for $3,000 or 1 x 5090 for $3500??â€",
                        "score": 106,
                        "replies": [
                            {
                                "id": "post_4_c2_r2_r1",
                                "text": "OK, I have to ask. For running models, do I take a hit using multiple cards in general? And I presume that is at least somewhat dependent on Motherboard as well.",
                                "score": 21,
                                "replies": [
                                    {
                                        "id": "post_4_c2_r2_r1_r1",
                                        "text": "Kind of. Ideally you want some kind of workstation/server class motherboard and chip with a boatload of pci-e lanes, that would be optimal. \n\nBut if you're just inferencing (generating outputs ie. text) then it doesn't really matter how many lanes each GPU has (similar to mining bitcoin), the data will move into the GPU's slower if the GPU is connected on a 4x slot/lane, but once the data is located in VRAM, then it's only a few % loss in inferencing speed compared to having full lane availability.\n\nWhere full lanes really matter is if you are fine-tuning or creating a model as there is so much chip to chip to communication (afaik).",
                                        "score": 49,
                                        "replies": [
                                            {
                                                "id": "post_4_c2_r2_r1_r1_r1",
                                                "text": "Can the model be parsed across them? Eg a model larger than 16gb split over 2x 16gb cards?",
                                                "score": 5,
                                                "replies": [
                                                    {
                                                        "id": "post_4_c2_r2_r1_r1_r1_r1",
                                                        "text": "It depends on architecture and inference engine and model format. For example, take GGUF format with llamma.cpp or kcpp backend. Lets say you have a 20 layer model, and two 8GB GPU's. For simplicity, lets say each layer uses 1GB of VRAM, you put 8 layers on one GPU, 8 layers on the other, the remaining layers (4) go into system RAM. So when you begin your inference forward pass through the model, the first GPU uses it's processing power on the 8 layers it has in it's own memory, the attention mechanism then moves into the second GPU, where it uses it's own processing power on it's 8 layers, finally the attention mechanism moves to the CPU of the computer where it passes through the 4 final layers in system RAM.\n\nThe first 8 layers are computed at the speed of the GPU, the second 8 layers are computed at the speed of the second GPU, the final 4 layers are processed at the speed of the CPU+RAM.\n\nThings to keep in mind.\n\n- splitting models across different GPU's/accelerators is known as \"tensor splitting\"\n\n- most of this shit only works as expected using Nvidia CUDA (although AMD ROCm and Vulkan are improving)\n\n- tensor splitting is unique to inference engines and formats (not all formats and engines can do this)\n\n- whenever possible, moving layers to a dedicated accelerator is in practice ALWAYS faster than using a CPU+RAM, hence why VRAM is king, CUDA is the boss, and Nvidia is worth a gajillion dollars\n\nTake all of this with a grain of salt, we're all novices in a field of computing that literally invalidates itself almost entirely every 6 months.",
                                                        "score": 33,
                                                        "replies": [
                                                            {
                                                                "id": "post_4_c2_r2_r1_r1_r1_r1_r1",
                                                                "text": "May the Lord bless you. This was so perfectly explained",
                                                                "score": 5
                                                            }
                                                        ]
                                                    }
                                                ]
                                            }
                                        ]
                                    },
                                    {
                                        "id": "post_4_c2_r2_r1_r2",
                                        "text": "No. Bandwidth between cards is only important when training.",
                                        "score": 7
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "id": "post_4_c2_r3",
                        "text": "Is this the place where we all huddle together and cry?",
                        "score": 42,
                        "replies": [
                            {
                                "id": "post_4_c2_r3_r1",
                                "text": "I'm hoping Nvidia reward the fanbois and don't take the complete piss with pricing.\n\nThey're making enough elsewhere, they don't need to ravage the enthusiast.",
                                "score": 20,
                                "replies": [
                                    {
                                        "id": "post_4_c2_r3_r1_r1",
                                        "text": "There's a strong argument at this point that the vast majority of their income comes from enterprise AI sales. The gaming and AI enthusiast market is nothing in comparison. Nvidia could stop selling gaming GPUs all together and their profit margins would barely notice.\n\nA savvy business decision however would be to continue to make and sell gaming cards for cheap as a kind of, \"your first hit is free\" kind of deal. Get people into CUDA, make them associate \"AI chip = Nvidia\", invest in the future.\n\n16 year old kids with pocket money who get a new GPU for Christmas go to college to study AI, graduate and set up their own home lab, become fat and bitter Redditors in their 30's working as senior engineers at major tech companies who have an AI harem in their basement. They're the guys who are making the decision which brand of GPU to buy for their corporate two hundred million dollar AI project. You want those guys to be die-hard Nvidia fanboys who swear nothing else is worth their time.\n\nCheap consumer cards are an investment in the future.",
                                        "score": 55,
                                        "replies": [
                                            {
                                                "id": "post_4_c2_r3_r1_r1_r1",
                                                "text": "Basement AI harem. Thats a first",
                                                "score": 22,
                                                "replies": [
                                                    {
                                                        "id": "post_4_c2_r3_r1_r1_r1_r1",
                                                        "text": "Y-yeah haha w-what a ridiculous totally fictional characture of a person.",
                                                        "score": 32,
                                                        "replies": [
                                                            {
                                                                "id": "post_4_c2_r3_r1_r1_r1_r1_r1",
                                                                "text": "Haha yeah, who would possibly even think of doing that? ðŸ˜…ðŸ‘€",
                                                                "score": 3,
                                                                "replies": [
                                                                    {
                                                                        "id": "post_4_c2_r3_r1_r1_r1_r1_r1_r1",
                                                                        "text": "Hahahahah\n\nHahah\n\nHahaha\n\nYeah...",
                                                                        "score": 3
                                                                    }
                                                                ]
                                                            }
                                                        ]
                                                    }
                                                ]
                                            },
                                            {
                                                "id": "post_4_c2_r3_r1_r1_r2",
                                                "text": "10:1 to be exact",
                                                "score": 6
                                            },
                                            {
                                                "id": "post_4_c2_r3_r1_r1_r3",
                                                "text": "The strategy you describe is exactly what they are doing, only the first consumer hit is not quite free, it's $2-3k.",
                                                "score": 4
                                            }
                                        ]
                                    },
                                    {
                                        "id": "post_4_c2_r3_r1_r2",
                                        "text": "They rule the world. They don't have to play nice for anyone.",
                                        "score": 6
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "id": "post_4_c2_r4",
                        "text": "Now that there's no high end competitor, nvidia can charge whatever they want.",
                        "score": 10
                    },
                    {
                        "id": "post_4_c2_r5",
                        "text": "$5080",
                        "score": 3
                    },
                    {
                        "id": "post_4_c2_r6",
                        "text": "2949.99 + tax\n\nEdit: If the A6000 stays the same price... 3500 is probably closer ;\\\\\n\nEdit2: 48gig = 4800, 32gig = 3200 bucks, if going by cost per gig and speed is ignored.\n\nEdit3: with o1prev's 2 cents.\n\nBased on the information you've provided and historical pricing trends, the NVIDIA GeForce RTX 5090 with 32â€¯GB of memory could be expected to be priced between **$2,500 and $3,000**. Here's how this estimate is derived:\n\n1. **Historical Pricing Trends**:\n   * The RTX 3090, with 24â€¯GB of memory, was priced between $1,000 and $1,300.\n   * The RTX 4090, also with 24â€¯GB, saw a significant price increase to around $2,000.\n   * This indicates a trend where flagship GPUs see substantial price jumps between generations.\n2. **Memory Capacity and Pricing**:\n   * The RTX 4090 is priced at approximately $83 per GB ($2,000/24â€¯GB).\n   * Applying a similar or slightly higher price per GB to the RTX 5090 (due to new technology and performance improvements) results in:\n      * $83 Ã— 32â€¯GB = $2,656\n      * Considering market factors and potential premium pricing, this could round up to between $2,500 and $3,000.\n3. **Comparison with Professional GPUs**:\n   * The NVIDIA A6000, a professional GPU with 48â€¯GB of memory, is priced at $4,800.\n   * While professional GPUs are typically more expensive due to additional features and optimizations for professional workloads, the pricing provides a ceiling for high-memory GPUs.\n\n**Conclusion**:\n\nGiven these factors, a reasonable estimate for the RTX 5090's price would be in the **$2,500 to $3,000** range. However, please note that this is a speculative estimate. The actual price could vary based on NVIDIA's pricing strategy, manufacturing costs, competition, and market demand at the time of release.",
                        "score": 21,
                        "replies": [
                            {
                                "id": "post_4_c2_r6_r1",
                                "text": "Jensen making PBJ in the kitchen \"hun, what do you think about just keeping the pricing simple and making it the same as the VRAM?\"",
                                "score": 12
                            },
                            {
                                "id": "post_4_c2_r6_r2",
                                "text": "At $3,000, Any reasonable person into gen AI will just spend the extra money and get a used 48gb a6000.  You get more vram for your money, and less power requirements.   The only reason to get 5090 will be if you are training/fine-tuning, but large scale training is out of reach we no longer dream of it.  At best we finetune, and I rather have more vram and a fine-tune that takes 2x longer than the other way around.",
                                "score": 7
                            },
                            {
                                "id": "post_4_c2_r6_r3",
                                "text": "The 4090 is $2000 new, if it goes out of stock, maybe the 5090 will be $2500, but eventually I see ot coming down to $2000",
                                "score": 4
                            },
                            {
                                "id": "post_4_c2_r6_r4",
                                "text": "Guess I am going AMD.",
                                "score": 4,
                                "replies": [
                                    {
                                        "id": "post_4_c2_r6_r4_r1",
                                        "text": "AMD has no interest in competing at the high end part of the market. And if Nvidia can profit from raising prices, AMD has shown to follow closely behind. They, too, are a publicly traded company, so donâ€™t expect them to do anything that would benefit their clientele.",
                                        "score": 5
                                    }
                                ]
                            }
                        ]
                    }
                ]
            },
            {
                "id": "post_4_c3",
                "text": "I think the memory bandwidth is wrong on VideoCardz. Should be 1792 GB/s if the memory chips are 28 Gbps as they list it, no?\n\nCompared to 4090 which has 1008 GB/s bandwidth, they claim 5090 has 1.333x the memory bus (512bit vs 384bit) and 1.333x faster memory chips (28 Gbps vs 21 Gbps).\n\n1.333 x 1.333 x 1008 GB/s = 1792 GB/S.",
                "score": 88,
                "replies": [
                    {
                        "id": "post_4_c3_r1",
                        "text": "Good eye! It seems like they calculated it for 448bit.\n\n28 / 8 x 448 = 1568  \n28 / 8 x 512 = 1792",
                        "score": 55,
                        "replies": [
                            {
                                "id": "post_4_c3_r1_r1",
                                "text": "448 bit would work out to 28GB total memory correct?",
                                "score": 8,
                                "replies": [
                                    {
                                        "id": "post_4_c3_r1_r1_r1",
                                        "text": "Yes, but kopite7kimi confirmed its 32GB, so its 512bit.",
                                        "score": 17,
                                        "replies": [
                                            {
                                                "id": "post_4_c3_r1_r1_r1_r1",
                                                "text": "512bit = happy dance time",
                                                "score": 4
                                            }
                                        ]
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "id": "post_4_c3_r2",
                        "text": "It's not totally out of the ordinary to use faster chips than what's actually used.\n\nThe 3090 for example shipped with 21Gbps memory but only clocked it at 19.5Gbps",
                        "score": 16
                    }
                ]
            },
            {
                "id": "post_4_c4",
                "text": "Nvidia give us at least 48GB!!!",
                "score": 40,
                "replies": [
                    {
                        "id": "post_4_c4_r1",
                        "text": "Like they give a shit when they can sell GDDR7 with insane premiums on datacenter cards.",
                        "score": 7
                    }
                ]
            },
            {
                "id": "post_4_c5",
                "text": "We desperately need a 3060 speed card with 24gb VRAM\n\nThat would be a perfect price point and usage sweet spot.",
                "score": 62,
                "replies": [
                    {
                        "id": "post_4_c5_r1",
                        "text": "Intel/AMD could grab some marketshare if they did this. They don't want to compete at the high end and they don't have to. Just be aggressive with memory at the mid-range. Label it AI-Ready, charge a small premium... that could work out pretty well for everyone.",
                        "score": 32,
                        "replies": [
                            {
                                "id": "post_4_c5_r1_r1",
                                "text": "Exactly, now is the best time to do it too",
                                "score": 8
                            },
                            {
                                "id": "post_4_c5_r1_r2",
                                "text": "And donate some to turboderp, the unsloth devs, etc",
                                "score": 3
                            },
                            {
                                "id": "post_4_c5_r1_r3",
                                "text": "Didn't Intel just go under?\n\nThe future is looking more and more like cyberpunk. Arasaka is a thinly-veiled metaphor for Nvidia.",
                                "score": 3
                            }
                        ]
                    },
                    {
                        "id": "post_4_c5_r2",
                        "text": "Strix Halo laptop, can have around 96GB allocated to VRAM and it's grunt power to around 4070M if not faster.",
                        "score": 5
                    },
                    {
                        "id": "post_4_c5_r3",
                        "text": "I think what you're describing is closest to the RTX A5000 (24GB) which if I recall correctly is also the ampere generation and has 3080-equivalent compute. But that goes for no less than $1500 used these days. A dual A4000 or T4 setup might make sense too, until they have some high vram / low compute inferencing cards in the market",
                        "score": 8
                    }
                ]
            },
            {
                "id": "post_4_c6",
                "text": "600W is nuts. I hope there's an easy way to limit this. That same 600W could power 1.62 3090s (realistically, two). People who were hitting slot limits are now just going to hit power limits instead.",
                "score": 78,
                "replies": [
                    {
                        "id": "post_4_c6_r1",
                        "text": "> That same 600W could power 1.62 3090s\n\nYou can limit 3090s power to less than 200w, but I guess you will be able to do the same with the 5090.",
                        "score": 26,
                        "replies": [
                            {
                                "id": "post_4_c6_r1_r1",
                                "text": "How do you limit it to 200W?\n\nEdit: sounds like afterburner will do it.",
                                "score": 3,
                                "replies": [
                                    {
                                        "id": "post_4_c6_r1_r1_r1",
                                        "text": "$ nvidia-smi -pl 200",
                                        "score": 10
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "id": "post_4_c6_r2",
                        "text": "You can limit the Watts in MSI Afterburner.",
                        "score": 9
                    },
                    {
                        "id": "post_4_c6_r3",
                        "text": "> 600W is nuts. I hope there's an easy way to limit this.\n\nI'm sure MSI Afterburner will work for the 5090 too. Nvidia overpowers its flagship GPUs by ridiculous amounts. I limit my 4090 from 450 to 350 W without any loss of performance.",
                        "score": 8,
                        "replies": [
                            {
                                "id": "post_4_c6_r3_r1",
                                "text": "4090 at 350w here too, cool and beastly! It might be a while but I'll do the same to a 5090 eventually...",
                                "score": 7
                            }
                        ]
                    },
                    {
                        "id": "post_4_c6_r4",
                        "text": "On linux there's an easy way with nvidia-smi. You can just tell it to have a different watt target and the card will abide (lowering freqs etc). Afaik it works with all Nvidia cards, I've tested it on 1060 and 3090. I'm running my 3090 with 200 Watts.",
                        "score": 19,
                        "replies": [
                            {
                                "id": "post_4_c6_r4_r1",
                                "text": "Annoyingly, one thing it _doesn't_ work on is mobile GPUs. I've had way too many gaming / workstation laptops that sound like jet engines under load as a result.\n\nFor my 4090, though, it's downright shocking how little performance I lose when I limit it to 50% power. (I use MSI Afterburner on Windows, but as others have noted you can also use the same command line tool, too).",
                                "score": 7
                            }
                        ]
                    },
                    {
                        "id": "post_4_c6_r5",
                        "text": "I undervolted my 3080 Ti from 350W to 220W with like 10% performance hit (if not less) so pretty sure next get GPUs would allow same thing",
                        "score": 3
                    },
                    {
                        "id": "post_4_c6_r6",
                        "text": "Us poor Americans are limited to 1500w from a standard wall outlet too :(.",
                        "score": 3
                    }
                ]
            },
            {
                "id": "post_4_c7",
                "text": "16 gb on 5080 will be the most disappointing news in this year",
                "score": 26
            },
            {
                "id": "post_4_c8",
                "text": "AMD has a big chance here! Come on, Lisa, don't fail to see it! This is like Intel's 4-core situation all over again, but with memory! You can have another Ryzen moment and gain enough market share to compete strongly 2-3 generations down the line. LLMs will be in games within 1 to 2 years. Memory will become even more relevant after that. Produce a mid-range GPU with RTX 4070 ti level performance but with 24GB of RAM, and you'll win market share from NVIDIA. Keep that formula for 3 generations, and 30% market share becomes a viable option. It's so easy to see!\n\n\n8700xt 16gb  at 4070 performance 299$\n\n8800xt 24gb at 4080 or slidly lower 399$\n\nLower the Profit per card but increase the market share  this will increase the incentives for developers to benefit AMD cards due to owning a bigger chunk of the market.\n\nNVdia is sleeping like intel was on the 4 cores for 10 years straight.",
                "score": 18,
                "replies": [
                    {
                        "id": "post_4_c8_r1",
                        "text": "I like AMD specs (W7800 with 32 GB, W7900 with 48 GB), but they're completely clueless when it comes to software - so many years passed, and we still don't have working GPU acceleration for PyTorch on Windows.",
                        "score": 4
                    }
                ]
            },
            {
                "id": "post_4_c9",
                "text": "32 GB of fast memory + just 2 slot width means that this will let me build an amazing 64 GB 2x5090 LLM rig. Now I just need to sell a kidney to afford it. And a kid.",
                "score": 39,
                "replies": [
                    {
                        "id": "post_4_c9_r1",
                        "text": "don't worry, after you get irradiated by nuclear powerplant needed to power those you will grow 3rd kidney",
                        "score": 27
                    },
                    {
                        "id": "post_4_c9_r2",
                        "text": "64 GB @1500 GB/s would be sweet. If you fill the 64GB completely then you can read it 23.43 times in one second. About 23 tokens per second would be the performance ceiling with a model of that size then.",
                        "score": 9
                    },
                    {
                        "id": "post_4_c9_r3",
                        "text": "So two 32GB GPUs running parallel models could handle up to 32B parameters in FP16 mode or 16B parameters in FP32 mode.",
                        "score": 7,
                        "replies": [
                            {
                                "id": "post_4_c9_r3_r1",
                                "text": "Or 70B at q6k or something like a reasonable person.",
                                "score": 27
                            },
                            {
                                "id": "post_4_c9_r3_r2",
                                "text": "Yes-ish. You can quantize LLMs and still have a very good model that fits in a lot less VRAM. [https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator](https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator)",
                                "score": 6
                            },
                            {
                                "id": "post_4_c9_r3_r3",
                                "text": "35B at 8 bits with plenty of space to spare for context and cache, or 70B at more aggressive quanitization",
                                "score": 3
                            }
                        ]
                    },
                    {
                        "id": "post_4_c9_r4",
                        "text": "The good news is making kids to sell is free!",
                        "score": 4
                    }
                ]
            },
            {
                "id": "post_4_c10",
                "text": "This will make previous generation cards cheaper right? Right?",
                "score": 74,
                "replies": [
                    {
                        "id": "post_4_c10_r1",
                        "text": "I dunno, 3090s have been really steady around me, if not even a bit more expensive for the past year (+~50eur depending on where you're buying from).",
                        "score": 52,
                        "replies": [
                            {
                                "id": "post_4_c10_r1_r1",
                                "text": "4090s in my country become 50% more expensive this year and ASUS ROG versions even two times expensive.",
                                "score": 28
                            },
                            {
                                "id": "post_4_c10_r1_r2",
                                "text": "Yep, same here in terms of observed price stability on 3090s. I am certain most of the demand is from enthusiasts in our domain rather than gamers. \n\nFor sure though supply of second hand 4090s will increase the amount of 24gb cards on the market, and there will be rich gamers who always want the 5090 flogging them. \n\nMy hope - 20% off current 3090 prices (which will take them to the price they were after the crypto crash) - and a decent supply of 4090s available for $800/Â£800/â‚¬800 used",
                                "score": 11,
                                "replies": [
                                    {
                                        "id": "post_4_c10_r1_r2_r1",
                                        "text": "Don't forget China wants 4090s to de-chip for whatever it is they do with them to beat the embargo.",
                                        "score": 8
                                    },
                                    {
                                        "id": "post_4_c10_r1_r2_r2",
                                        "text": "A used 3090 is the cheapest card for deep learning in general, with a decent amount of GDDR memory. The 24GB will keep it that way for another few years I suppose.",
                                        "score": 7
                                    },
                                    {
                                        "id": "post_4_c10_r1_r2_r3",
                                        "text": "Really hope youâ€™re right. Need one or ideally two 3090s, and right now itâ€™s a struggle to save up enough with no disposable income after all expenses.",
                                        "score": 4
                                    }
                                ]
                            },
                            {
                                "id": "post_4_c10_r1_r3",
                                "text": "Got extremelyyyyy lucky, found a second one for 350â‚¬, and it works fine, just a display port not working , but I couldn't care less for that price",
                                "score": 6,
                                "replies": [
                                    {
                                        "id": "post_4_c10_r1_r3_r1",
                                        "text": "Damn. At that price I'd be buying 4.",
                                        "score": 6
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "id": "post_4_c10_r2",
                        "text": "Once the entire stock is instantly bought by bots, it will be as if it never came out at all.",
                        "score": 31,
                        "replies": [
                            {
                                "id": "post_4_c10_r2_r1",
                                "text": "Bitch, so right. Literal lol!",
                                "score": 3
                            }
                        ]
                    },
                    {
                        "id": "post_4_c10_r3",
                        "text": "The \\*only\\* thing that can make previous gen cards cheaper is competitive pressure from AMD.  There is a 0% chance that NVIDIA will look at the current market and price these in any way that would undercut their own current top end (i.e. if 4090-level performance is currently $2000 and a 5090 is 30% faster, it will cost AT LEAST 30% more, etc.)",
                        "score": 13,
                        "replies": [
                            {
                                "id": "post_4_c10_r3_r1",
                                "text": "Well, the 4090 is going out of stock, they already planning to discontinue it",
                                "score": 3,
                                "replies": [
                                    {
                                        "id": "post_4_c10_r3_r1_r1",
                                        "text": "Sure.. but there is going to be a 5xxx performance-equivalent to a 4090 (e.g. typically the xx80 card of the next generation will be within a few % of the xx90's of the previous generation).  THAT card will be priced at the equivalent price to a current 4090 AND if the 5080 does indeed only have 16GB RAM, then all bets are off on that release driving down used 4090 prices even a single penny.    \n\nAgain, why would NVIDIA ever price things to eat into their own profit if there is currently zero external market pressure to do so?  Nobody at the top (whether it were Intel, AMD, or NVIDIA has ever competed with themselves unnecessarily on price when they were on top).  The halo product is going to extract halo prices.",
                                        "score": 3
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "id": "post_4_c10_r4",
                        "text": "Lol, nope.Â  3090s are still liek $900.Â  I think 4090s have appreciated in price.Â  And enough of the folks buying these things, i.e. enthusiasts with money to burn who likely work in tech roles where they are buying gpus like hotcakes or have gpu access in the cloud, will be able to shell out to make it profitable for Nvidia.Â Â \n\n\nThey also dont want the poors to have this tech unless its through an API.",
                        "score": 6
                    },
                    {
                        "id": "post_4_c10_r5",
                        "text": "Depends on price of RTX 5090. \n\nIf it will be roughly at the same price as RTX 4090 and there wont be issues with pricing then price of RTX 4090 on used market will drop a lot (20%+). \n\nIf its way more expensive than RTX 4090 then pricing of RTX 4090 on used market will drop just by 10% (because there will be a lot bigger supply as people upgrade to RTX 5090 and bunch of upgraders will sell 4090 at lower than market price to sell it instantly).",
                        "score": 4
                    }
                ]
            },
            {
                "id": "post_4_c11",
                "text": "That is not the model number it is the MSRP",
                "score": 18
            },
            {
                "id": "post_4_c13",
                "text": "Is the card even bigger (in physical size) than the 4090? I'm already having to use an anti-sag tool on my 4090 and had to upgrade to a bigger tower to fit it. It's heavy as hell.",
                "score": 13,
                "replies": [
                    {
                        "id": "post_4_c13_r1",
                        "text": "When I got my 4090 I got an anti-sag tool because how much a beast it is.  I think we are at a point that desktops need a fundamental redesign and instead of putting video cards on the inside, desktops motherboards should have a connector on the outside and instead of them being just video cards, they should be a self contained cooling box with its own separate power supply that connects to the side of a desktop instead of inside it.  That would solve so many issues, no longer suffer size restrictions, or needing to upgrade your power supply.",
                        "score": 14,
                        "replies": [
                            {
                                "id": "post_4_c13_r1_r1",
                                "text": "I remember installing mine thinking to myself can you imagine if we were still using sound cards, NIC cards, floppy drives, CD-ROM's, and/or HD spin drives, etc.? The weight of the case would be massive. Everything has scaled down, but NVIDIA keeps getting bigger.",
                                "score": 6
                            }
                        ]
                    },
                    {
                        "id": "post_4_c13_r2",
                        "text": "Article that I've posted mentions size  \n\"Interestingly, although the power has increased from 450W to 600W, the card is still said to feature a 2-slot cooler (Founders Edition). This likely means that NVIDIA is using some kind of non-standard cooling design, perhaps a liquid cooler.Â \"",
                        "score": 11
                    }
                ]
            },
            {
                "id": "post_4_c21",
                "text": "Any particular reason nobody here seems to look at the AMD cards? \n\nI've been using a 7900XT with ROCm on Linux with no issues. 20Gb/$700. The 7900XTX has 24Gb, and runs about $1000.\n\nI'm not chasing tokens/sec, I admit. It's been plenty fast, though.",
                "score": 18,
                "replies": [
                    {
                        "id": "post_4_c21_r1",
                        "text": "Well for me Nvidia has one benefit - it always works.  \nIt's great that you can run some LLMs with ROCm, but if you like to play with new stuff its always CUDA-first and then you wait and wait until someone manages to port it over ROCm or it never gets ported.\n\nFor example last month I added captions to all my movies using WhisperX - there's only CUDA and CPU to choose. Can I choose different Whisper implementation instead of WhisperX? Yea, I can spend hour trying to find something that works, then have no docs or help online because virtually nobody uses that and then, when I'll get this working it will be 10x slower than WhisperX implementation.\n\nNo matter what comes next, if you want to play with it be prepared to wait, because AMD just doesn't invest in their ecosystem enough so until it gets traction there won't be any port, it will CUDA-only.\n\n  \nOpenAI, Microsoft etc. use only Nvidia hardware to do all stuff, because Nvidia invested heavily in their ecosystem and Nvidia has clear vision. AMD lacks that vision, their engineers make a good product, their marketing team has fuckups everytime they touch anything (Ryzen 9000 release clearly showed how bad AMD marketing team is, bad reviews for good product, all because marketing hyped it way too much) and then they have no idea how many years they will support that - its like they would toss a coin to see how many years it will be alive. Nvidia has CUDA from... 2007? They didnt even change name.",
                        "score": 42,
                        "replies": [
                            {
                                "id": "post_4_c21_r1_r1",
                                "text": ">For example last month I added captions to all my movies using WhisperX - there's only CUDA and CPU to choose\n\nI ported CTranslate2 over to ROCm a while ago so faster-whisper and whisperX now [work on ROCm](https://github.com/arlo-phoenix/CTranslate2-rocm/blob/rocm/README_ROCM.md#whisperx)",
                                "score": 18,
                                "replies": [
                                    {
                                        "id": "post_4_c21_r1_r1_r1",
                                        "text": "That's amazing! I found CTranslate2 to be the best backend. WhisperS2T has TensorRT backend option, its 2x faster, but it worsens quality, so I always pick CTranslate2.\n\nBut you see - the problem is that no one knows that you did such amazing work. If I go to WhisperX github page there is only mention of CUDA and CPU. If I Google \"WhisperX ROCm\" there's nothing.\n\nIf AMD would hire just one Technical Writer that would write on AMD blog about ROCm implementations, ports and cool stuff that would be doing wonders. It's so easy for them to make their ecosystem \"good enough\", but they don't do anything in terms of promoting ROCm or make it more accessible.",
                                        "score": 15
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "id": "post_4_c21_r2",
                        "text": "native CUDA support",
                        "score": 33
                    },
                    {
                        "id": "post_4_c21_r3",
                        "text": "Does it work well? The truth is that I bought an nvidia gpu because of the damn CUDA. Sadly all AI shit is optimized for that environment. Also for gaming, dlss and rtx.",
                        "score": 5,
                        "replies": [
                            {
                                "id": "post_4_c21_r3_r1",
                                "text": "For my use case it works great. I'm using ollama's rocm docker image. \n\nRuns llama 3.1 pretty quickly, much faster than the GGUF on my 3070Ti (8Gb, so no surprise).\n\nI'm not doing any particular research, I just don't want to be paying a monthly fee. FWIW, it runs Cyberpunk 2077 (don't judge me!) really well, too.",
                                "score": 8,
                                "replies": [
                                    {
                                        "id": "post_4_c21_r3_r1_r1",
                                        "text": "There's nothing there to judge, Cyberpunk is one of the best games that have come out in the last decade",
                                        "score": 5
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "id": "post_4_c21_r4",
                        "text": "Tinycorp had a lot of problems with AMD cards for AI workloads. I'm not sure how common that is. https://x.com/__tinygrad__",
                        "score": 6
                    },
                    {
                        "id": "post_4_c21_r5",
                        "text": "Lack of CUDA makes thing really flakey. Nvidia is guaranteed to run.",
                        "score": 4
                    },
                    {
                        "id": "post_4_c21_r6",
                        "text": "> I'm not chasing tokens/sec\n\nMost of us are. Everything below 10t/s is agonizing.",
                        "score": 12,
                        "replies": [
                            {
                                "id": "post_4_c21_r6_r1",
                                "text": "At the risk of starting an argument about t/s benchmarks, I found a simple python script for testing ollama tokens/sec.\nhttps://github.com/MinhNgyuen/llm-benchmark\n\nI got this:\n----------------------------------------------------\n        llama3.1:latest\n        \tPrompt eval: 589.15 t/s\n        \tResponse: 87.02 t/s\n        \tTotal: 89.05 t/s\n\n        Stats:\n        \tPrompt tokens: 19\n        \tResponse tokens: 690\n        \tModel load time: 0.01s\n        \tPrompt eval time: 0.03s\n        \tResponse time: 7.93s\n        \tTotal time: 8.02s\n----------------------------------------------------\n\nIt's far from \"agonizing\"",
                                "score": 3,
                                "replies": [
                                    {
                                        "id": "post_4_c21_r6_r1_r1",
                                        "text": "I don't really give a shit about benchmarks. Show me the t/s of a real conversation with 32k context.",
                                        "score": 10,
                                        "replies": [
                                            {
                                                "id": "post_4_c21_r6_r1_r1_r1",
                                                "text": "Do you have an example of a \"real conversation\", and how to measure it? \n\nI use it all the time. I don't have any complaints about the performance. I find it very usable.\n\nI also have $1300 that I wouldn't have had otherwise. I could buy another card, put it in another server, and still have $600 - almost enough for yet another card.",
                                                "score": 3,
                                                "replies": [
                                                    {
                                                        "id": "post_4_c21_r6_r1_r1_r1_r1",
                                                        "text": "I use ooba as my backend and there I can see the t/s for every generation. Your backend should show this to you too. The longer the context the slower the generation typically, so it's important to test with a high context (at least for me, since thats what I'm using).\n\nAlso the model size is important. Small models are much faster than big ones.\n\nI'm also not sure I can follow what you mean with the money talk.",
                                                        "score": 8
                                                    }
                                                ]
                                            }
                                        ]
                                    }
                                ]
                            }
                        ]
                    }
                ]
            },
            {
                "id": "post_4_c35",
                "text": "24GB -> 32GB jump is huge.\n\nSome models that required you to get two 3090 or 4090 will now be able to run on single card. Of course its not 48GB like dual 3090/4090, but 32GB is still solid upgrade and I'm very happy that we will get 32GB instead of 28GB that was rumored earlier by some other leakers.\n\nThis leaker has very good accuracy, so I would say 32GB is confirmed.\n\nAlso, 50% memory bandwidth upgrade is absolutely INSANE. It's basically 1.5x of RTX 4090.",
                "score": 10,
                "replies": [
                    {
                        "id": "post_4_c35_r1",
                        "text": "the titan rtx already had 24gb. the 3090 had 24gb. the 4090 had 24gb.\n\nafter 3 generations we finally get an upgrade, and its just 33%? no, this is not \"huge\". there is little reason to buy one of these, compared to two 3090s.",
                        "score": 93,
                        "replies": [
                            {
                                "id": "post_4_c35_r1_r1",
                                "text": ">compared to two 3090s.\n\nmore like 4 or 5, maybe even 6. the 5090 will be eye-wateringly expensive",
                                "score": 15,
                                "replies": [
                                    {
                                        "id": "post_4_c35_r1_r1_r1",
                                        "text": "iâ€™m hoping there are going to be some special capabilities that continue the trend of higher efficiency from newer architectures, especially for doing things like quantized inference and various other blood theyâ€™re able to squeeze from the stone with the TensorRT black magic. 3090 extremely likely to continue to present good value though as it seems unlikely that architectural innovations could bring huge gains in fp16/32 training throughput. caveat this with the fact that actually i know nothing about that.",
                                        "score": 3
                                    },
                                    {
                                        "id": "post_4_c35_r1_r1_r2",
                                        "text": "The problem with buying 4 or 5 is to find a Motherboard that acutally lets you put them in.",
                                        "score": 3
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "id": "post_4_c35_r2",
                        "text": "...and yet still not nearly enough...",
                        "score": 8
                    },
                    {
                        "id": "post_4_c35_r3",
                        "text": "Iâ€™m disappointed, just gonna say it. Â This is the 5090. Â It should have had 48GB minimum, and ideally 64GB. Â ",
                        "score": 12,
                        "replies": [
                            {
                                "id": "post_4_c35_r3_r1",
                                "text": "Wishful thinking on your part. This card is for gaming, gaming card price != ai card price. They wont cut their profits.",
                                "score": 9
                            }
                        ]
                    },
                    {
                        "id": "post_4_c35_r4",
                        "text": "70b Q4 needs 35gb of VRAM without factoring context length. 32gb doesn't really raise the bar much. 40gb of VRAM gives room to run a standard Q4 with a fair amount of context once excluding the OS eating up some VRAM which can be remedied by using the motherboard for display out if you got integrated graphics. Most boards aren't supporting a lot of displays for that.\n\nSpeed is a whole different story but I get 40gb VRAM using my 4060 Ti + P40",
                        "score": 3
                    }
                ]
            }
        ]
    },
    {
        "id": "post_5",
        "url": "https://i.redd.it/b1wiktivh8rd1.png",
        "title": "Did Mark just casually drop that they have a 100,000+ GPU datacenter for llama4 training?",
        "upvotes": 604,
        "rank": 5,
        "comments": [
            {
                "id": "post_5_c1",
                "text": "Gates said something about how datacenters used to be measured by processors and now they are measured by megawatts.",
                "score": 332,
                "replies": [
                    {
                        "id": "post_5_c1_r1",
                        "text": "People saying AI is a bubble yet we are talking the same power input os entire countr**ies** in the future.",
                        "score": 148,
                        "replies": [
                            {
                                "id": "post_5_c1_r1_r1",
                                "text": "To be fair some of these large AI companies have more revenue than the GDP of multiple countries combined, not to mention vastly more influence on global culture.",
                                "score": 144,
                                "replies": [
                                    {
                                        "id": "post_5_c1_r1_r1_r1",
                                        "text": "Thatâ€™s literally their entire point.",
                                        "score": 52
                                    },
                                    {
                                        "id": "post_5_c1_r1_r1_r2",
                                        "text": "No \"to be fair\" about it. A country and a company are not comparable, just because they have a similar amount of money sloshing around.\n\nMay as well say a diamond ring is as good as a car.",
                                        "score": 13,
                                        "replies": [
                                            {
                                                "id": "post_5_c1_r1_r1_r2_r1",
                                                "text": "To be fair, a diamond ring is only good if you already have a car.",
                                                "score": 3
                                            }
                                        ]
                                    }
                                ]
                            },
                            {
                                "id": "post_5_c1_r1_r2",
                                "text": "Yes but bubble is about output, not input.\n\nNo one can say that the big tech arenâ€™t buying loads of accelerators and training impressive models.\n\nThe question is, will that flow through into real economic benefit: more bread, more corn, more t-shirts, or real digital goods people will pay for, and will the amount people are willing to pay for them exceed the cost to train and run the models?\n\nIf it doesnâ€™t, this IS a bubble. Itâ€™s just that the big techs are part of the â€œbag holdersâ€ this time. Theyâ€™ll be stuck with a huge amount of AI and no way to turn it into $$.",
                                "score": 11,
                                "replies": [
                                    {
                                        "id": "post_5_c1_r1_r2_r1",
                                        "text": "MORE DIGITIAL CORNBREAD T-SHIRTS!",
                                        "score": 5
                                    }
                                ]
                            },
                            {
                                "id": "post_5_c1_r1_r3",
                                "text": "Crypto energy usage was also comparable to the amount used by countries.",
                                "score": 37
                            },
                            {
                                "id": "post_5_c1_r1_r4",
                                "text": "I'm not saying it's a bubble but those two things aren't mutually exclusive",
                                "score": 19
                            },
                            {
                                "id": "post_5_c1_r1_r5",
                                "text": "Itâ€™s far better than mining though, at least AI makes life easier for everyone.",
                                "score": 23,
                                "replies": [
                                    {
                                        "id": "post_5_c1_r1_r5_r1",
                                        "text": "Well, it has way more fields, uses, prospects... Its an actual product, and its going to be everywhere, cant compare these two.",
                                        "score": 10
                                    }
                                ]
                            },
                            {
                                "id": "post_5_c1_r1_r6",
                                "text": "The most obvious sign that AI is a bubble (or will be given current tech) is that the main source of improvements *is* to use the power input of entire countries.\n\nIf AI hypothetically goes far beyond where it is now, it wonâ€™t be through throwing more power and vram at it.",
                                "score": 3
                            }
                        ]
                    },
                    {
                        "id": "post_5_c1_r2",
                        "text": "It's true, they are limited by access to the grid and cooling. [One B200 server rack runs you half a megawatt.](https://docs.nvidia.com/https:/docs.nvidia.com/nvidia-dgx-superpod-data-center-best-practices-with-dgx-b200.pdf)",
                        "score": 4
                    },
                    {
                        "id": "post_5_c1_r4",
                        "text": "Semi-Automatic Ground Environment (SAGE) would like to have a word.\n\nhttps://en.wikipedia.org/wiki/AN/FSQ-7_Combat_Direction_Central",
                        "score": 3
                    }
                ]
            },
            {
                "id": "post_5_c2",
                "text": "Llama 4 coming soon",
                "score": 94,
                "replies": [
                    {
                        "id": "post_5_c2_r1",
                        "text": "Llama ~~3.1~~ 3.2 feels like it came out just yesterday, damn this field is going at light speed.\n\nAny conjecture as to when or where about Llama 4 might drop.\n\nI'm really excited to see the story telling finetunes that will come out after...\n\nEdit: got the ver num wrong... mb.",
                        "score": 64,
                        "replies": [
                            {
                                "id": "post_5_c2_r1_r1",
                                "text": "Bro lama 3.2 did just come out yesterday ðŸ™ƒ",
                                "score": 108,
                                "replies": [
                                    {
                                        "id": "post_5_c2_r1_r1_r1",
                                        "text": "We have llama 3.2 already???",
                                        "score": 26,
                                        "replies": [
                                            {
                                                "id": "post_5_c2_r1_r1_r1_r1",
                                                "text": "You guys have llama 3.1???",
                                                "score": 10,
                                                "replies": [
                                                    {
                                                        "id": "post_5_c2_r1_r1_r1_r1_r1",
                                                        "text": "Wait, what? Why am I still using Llama-2?",
                                                        "score": 6,
                                                        "replies": [
                                                            {
                                                                "id": "post_5_c2_r1_r1_r1_r1_r1_r1",
                                                                "text": "Because Miqu model is still fantastic",
                                                                "score": 3
                                                            }
                                                        ]
                                                    }
                                                ]
                                            }
                                        ]
                                    }
                                ]
                            }
                        ]
                    }
                ]
            },
            {
                "id": "post_5_c3",
                "text": "The engineering team released in a blog post last year that they will have 600,000 by the end of this year.\n\n  \nAmdahl's law means that it doesn't mean they will necessarily be able to network and effectively utilize all that at once in a single cluster.\n\nIn fact llama 3.1 405B was pre-trained on a 16,000 H100 gpu cluster.",
                "score": 111,
                "replies": [
                    {
                        "id": "post_5_c3_r1",
                        "text": "Yeah the article that showed the struggles they overcame for their 25,000 h100 GPU clusters was really interesting. Hopefully they release a new article with this new beast of a data center and what they had to do for efficient scaling with 100,000+ GPUs. At that number of gpus there has to be multiple gpus failing each day and I'm curious how they tackle that.",
                        "score": 39,
                        "replies": [
                            {
                                "id": "post_5_c3_r1_r1",
                                "text": "According to the llama paper they do some sort of automated restart from checkpoint. 400+ times in just 54 days. Just incredibly inefficient at the moment",
                                "score": 26,
                                "replies": [
                                    {
                                        "id": "post_5_c3_r1_r1_r1",
                                        "text": "Yeah do you think that would scale with 10 times the number of GPUs? 4,000 restarts?? No idea how long a restart takes but that seems brutal.",
                                        "score": 11,
                                        "replies": [
                                            {
                                                "id": "post_5_c3_r1_r1_r1_r1",
                                                "text": "At this scale, reliability becomes as much of a deal as VRAM. Groq is cooperating with Meta, I suspect this may not be your commoner H100 that ends up in their 1M GPU cluster.",
                                                "score": 5
                                            },
                                            {
                                                "id": "post_5_c3_r1_r1_r1_r2",
                                                "text": "I don't think restart counts scale linearly with size, but probably logarithmically. You might have 800 restarts, or 1200. A lot of investment goes to keeping that number as low as possible. \n\nNvidia, truth be told, ain't nearly the perfectionist they make themselves out to be. Even their premium, top-tier GPUs have flaws.",
                                                "score": 9,
                                                "replies": [
                                                    {
                                                        "id": "post_5_c3_r1_r1_r1_r2_r1",
                                                        "text": "restarts due to hardware failures can be approximated by an exponential distribution, which does have linear mtbf scaling to number of hardware units",
                                                        "score": 13,
                                                        "replies": [
                                                            {
                                                                "id": "post_5_c3_r1_r1_r1_r2_r1_r1",
                                                                "text": "Good to know!",
                                                                "score": 5
                                                            }
                                                        ]
                                                    }
                                                ]
                                            }
                                        ]
                                    }
                                ]
                            },
                            {
                                "id": "post_5_c3_r1_r2",
                                "text": "In short, kubernetes.\n\nAlso a fuckload of preflight testing, burn in, and preemptively killing anything that even starts to look like it's thinking about failing.\n\nThat plus continuous checkpointing and very fast restore mechanisms.\n\nThat's not even the fun part, the fun part is turning the damn thing on without bottlenecking *literally everything.*",
                                "score": 13
                            },
                            {
                                "id": "post_5_c3_r1_r3",
                                "text": "Mind linking that article? I, in turn, could recommend this one by SemiAnalysis from June, even the free part is very interesting: https://www.semianalysis.com/p/100000-h100-clusters-power-network",
                                "score": 4
                            }
                        ]
                    },
                    {
                        "id": "post_5_c3_r2",
                        "text": "600k is metas entire fleet, including Instagram and Facebook recommendations and reels inference.\n\nIf they wanted to use all of it I'm sure they could get some downtime on their services, but it's looking like they will cross 1,000,000 in 2025 anyway",
                        "score": 18,
                        "replies": [
                            {
                                "id": "post_5_c3_r2_r1",
                                "text": "I think the majority of that infra will be used for serving, but gradually Meta is designing and fabbing its own inference chips. Not to mention there are companies like Groq and Cerebras that are salivating at the mere opportunity to ship some of their inference chips to a company like Meta.\n\nWhen those inference workloads get offloaded to dedicated hardware, there's gonna be a lot of GPUs sitting around just rarin' to get used for training some sort of ungodly scale AI algorithmns.  \n\n\nNot to mention the B100 and B200 blackwell chips haven't even shipped yet.",
                                "score": 7
                            }
                        ]
                    }
                ]
            },
            {
                "id": "post_5_c4",
                "text": "Wasnâ€™t it already public knowledge that they bought like 15,000 H100s? Of course theyâ€™d have a big datacenter",
                "score": 44,
                "replies": [
                    {
                        "id": "post_5_c4_r1",
                        "text": "Yes, public knowledge that they will have 600,000 H100 equivalents by the end of the year. However having that many GPUs is not the same as efficiently networking 100,000 into a single cluster capable of training a frontier model. In May they announced their dual 25k H100 clusters, but no other official announcements. The power requirements alone are a big hurdle. Elons 100K cluster had to resort to I think 12 massive portable gas generators to get enough power.",
                        "score": 35
                    },
                    {
                        "id": "post_5_c4_r2",
                        "text": "It is kinda weird that Facebook does not launch their own public cloud.",
                        "score": 10,
                        "replies": [
                            {
                                "id": "post_5_c4_r2_r1",
                                "text": "Seriously. What the fuck are they doing with that much compute?",
                                "score": 13,
                                "replies": [
                                    {
                                        "id": "post_5_c4_r2_r1_r1",
                                        "text": "[https://huggingface.co/collections/meta-llama/llama-32-66f448ffc8c32f949b04c8cf](https://huggingface.co/collections/meta-llama/llama-32-66f448ffc8c32f949b04c8cf)",
                                        "score": 5
                                    },
                                    {
                                        "id": "post_5_c4_r2_r1_r2",
                                        "text": "Signaling the lizard planet.",
                                        "score": 3
                                    }
                                ]
                            },
                            {
                                "id": "post_5_c4_r2_r2",
                                "text": "It's all about profit margins. Meta ads is a literal money printer. There is way less margin in public cloud. If they were to pivot into that, they'd need to spend years generalizing as internal infra is incredibly Meta-specific. And, they'd need to take compute away from the giant clusters they're building...",
                                "score": 11
                            }
                        ]
                    }
                ]
            },
            {
                "id": "post_5_c5",
                "text": "I was just at Pytorch Con, a lot is improving on the SW side as well to enable scaling past what we've gotten out of standard data and tensor parallel methods",
                "score": 11,
                "replies": [
                    {
                        "id": "post_5_c5_r1",
                        "text": "Anything specific?Â ",
                        "score": 3
                    }
                ]
            },
            {
                "id": "post_5_c6",
                "text": "See the interview here: [https://www.youtube.com/watch?v=oX7OduG1YmI](https://www.youtube.com/watch?v=oX7OduG1YmI)  \nI have to assume llama 4 training has started already, which means they must have built something beyond their current [dual 25k H100 datacenters](https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/).",
                "score": 16
            },
            {
                "id": "post_5_c7",
                "text": "He dropped it a while ago:\n\n[https://www.perplexity.ai/page/llama-4-will-need-10x-compute-wopfuXfuQGq9zZzodDC0dQ](https://www.perplexity.ai/page/llama-4-will-need-10x-compute-wopfuXfuQGq9zZzodDC0dQ)",
                "score": 10
            },
            {
                "id": "post_5_c8",
                "text": "Newbie here. Would using these newer trained models take the same resources, given that the llm is the same size?\n\nFor example, would llama3.2 7b and llama4 7b, require about the same resources and work at about the same speed? The assumption is that llama4 wouldnhave a 7b version and be roughly the same MB size.",
                "score": 10,
                "replies": [
                    {
                        "id": "post_5_c8_r1",
                        "text": "It depends... on a lot of things.\n\nFirst of all, the parameter count (7B) is sometimes rounded.\n\nSecond, some models use more vram for the context than others, though if you keep the context very small (like 1K) this isn't an issue.\n\nThird, some models *quantize* more poorly than others. This is more of a \"soft\" factor that effectively makes the models a little bigger.\n\nIt's also possible the architecture will change dramatically (eg be mamba + transformers, bitnet, or something) which could dramatically change the math.",
                        "score": 9
                    },
                    {
                        "id": "post_5_c8_r2",
                        "text": "Yes if they are the same architecture and the same number of parameters and if we were just talking dense models they are going to take the same number of resources. There's more complexity to answer but in general this holds true.",
                        "score": 4
                    }
                ]
            }
        ]
    },
    {
        "id": "post_6",
        "url": "https://www.reddit.com/r/LocalLLaMA/comments/1fselw2/newsom_vetoed_sb1047/",
        "title": "Newsom vetoed SB-1047!",
        "upvotes": 592,
        "rank": 6,
        "comments": [
            {
                "id": "post_6_c1",
                "text": "Yay, drop the Llama3.3 1050B q0.1_K_S boys, my laptop is ready",
                "score": 126,
                "replies": [
                    {
                        "id": "post_6_c1_r1",
                        "text": "4TB of RAM means I can run it on my laptop's CPU ðŸ˜ˆ",
                        "score": 17,
                        "replies": [
                            {
                                "id": "post_6_c1_r1_r1",
                                "text": "You could run it, but will it produce anything useful before heat-death of universe? doubtful.",
                                "score": 11,
                                "replies": [
                                    {
                                        "id": "post_6_c1_r1_r1_r1",
                                        "text": "A set of pens and few notebooks means I can calculate run the models by hand ðŸ˜ˆ",
                                        "score": 12
                                    },
                                    {
                                        "id": "post_6_c1_r1_r1_r2",
                                        "text": "If it spits out a useful answer to the Ultimate Question in 7.5 million years, it'll be worth the wait.",
                                        "score": 6
                                    }
                                ]
                            }
                        ]
                    }
                ]
            },
            {
                "id": "post_6_c2",
                "text": "Happy that Newsom listened to the many requests including mine to Veto this bill.",
                "score": 42,
                "replies": [
                    {
                        "id": "post_6_c2_r1",
                        "text": "I wrote to my state reps and Newsom to oppose it. He definitely listened to me. You're all welcome.",
                        "score": 17
                    },
                    {
                        "id": "post_6_c2_r2",
                        "text": "Every little bit counted, he took his time with this one so it was not an instant yes/no.",
                        "score": 10
                    },
                    {
                        "id": "post_6_c2_r3",
                        "text": "apparently he vetoed it because he wants even stricter regulation. its too early to celebrate.",
                        "score": 6
                    },
                    {
                        "id": "post_6_c2_r4",
                        "text": "thanks for actually taking the time and effort.",
                        "score": 5
                    }
                ]
            },
            {
                "id": "post_6_c3",
                "text": "Huh, I'm a bit surprised.  Hopefully that discourages further legislation along these lines.",
                "score": 45,
                "replies": [
                    {
                        "id": "post_6_c3_r1",
                        "text": "Hell no, you know round 2 is coming up just because.",
                        "score": 4
                    }
                ]
            },
            {
                "id": "post_6_c4",
                "text": "Damn, actually pretty happy with some decisions Newsom has been making. This and the speed limit nanny in new cars. Glad heâ€™s stopping some stupid laws from going through.",
                "score": 131,
                "replies": [
                    {
                        "id": "post_6_c4_r1",
                        "text": "read the article, while it's good, if he's being honest about his reasoning, it's because he wants regulation that will apply to all models, not just big ones... so this may just be a pyrrhic victory.\n\nAt least this dumb shit isn't happening \n\n>Developers also would have needed to ensure their AI could be shut down by a human if it started behaving dangerously.\n\nDon't need stupid science fiction nonsense being applied in law",
                        "score": 85,
                        "replies": [
                            {
                                "id": "post_6_c4_r1_r1",
                                "text": "If you read his actual statement, one of his core points is that you should not ban the model itself, but instead bad an application that does bad things with the model. That to me makes a ton of sense. Regulating software devoid of a use case is almost always a stupid idea.",
                                "score": 48,
                                "replies": [
                                    {
                                        "id": "post_6_c4_r1_r1_r1",
                                        "text": "Rest assured that there is more to this than has been made public. There is visible lobbying such as Altman testifying in Congress, and then there's all the stuff that is happening behind the scenes. I'm sure that Newsom has spoken privately to people like Zuckerberg, Nadella etc., who may have offered different perspectives (and incentives) from those that OpenAI has been pushing for.",
                                        "score": 18,
                                        "replies": [
                                            {
                                                "id": "post_6_c4_r1_r1_r1_r1",
                                                "text": "Wasn't Altman against the bill? That might have had something to do with the vetoing.",
                                                "score": 3,
                                                "replies": [
                                                    {
                                                        "id": "post_6_c4_r1_r1_r1_r1_r1",
                                                        "text": "[They believe AI regulation ](https://www.newsweek.com/openai-workers-push-california-ai-bill-against-sam-altman-1952033), particularly related to security and the U.S., should be addressed at the federal level rather than through state legislation",
                                                        "score": 4
                                                    }
                                                ]
                                            }
                                        ]
                                    }
                                ]
                            },
                            {
                                "id": "post_6_c4_r1_r2",
                                "text": "Just gives more time to decentralized. It's good to have warnings.",
                                "score": 23
                            }
                        ]
                    },
                    {
                        "id": "post_6_c4_r2",
                        "text": "Feels very uncharacteristic.  I wonder if this is the beginning of a calculated push to appear moderate, which will eventually pave the way for his 2028 presidential campaign bid",
                        "score": 20,
                        "replies": [
                            {
                                "id": "post_6_c4_r2_r1",
                                "text": "4 years is a long time. I donâ€™t think anyone would remember this the next election cycle. I think the reasons are less cynical than that. He might just believe what he says?",
                                "score": 16
                            },
                            {
                                "id": "post_6_c4_r2_r2",
                                "text": "It's actually rather characteristic of Newsom. He's generally leaned far more to the center than the legislature. I'm pretty sure he hails from the Gov Brown type Democrat.",
                                "score": 5
                            },
                            {
                                "id": "post_6_c4_r2_r3",
                                "text": "His veto against a recent privacy bill wouldn't help in the public eye.  Of course, that favor might be for the sake of getting funding from the corporations.",
                                "score": 5
                            },
                            {
                                "id": "post_6_c4_r2_r4",
                                "text": "I had expected him to see SB 1047 as an accomplishment which would help him in 2028, but am glad to be wrong.",
                                "score": 8
                            },
                            {
                                "id": "post_6_c4_r2_r5",
                                "text": "Incredibly unlikely, unless Kamala loses. \n\nSerious challenges to the incumbent candidate just don't really happen. \n\nNo incumbent in modern history has ever lost the primary when running for a second term.",
                                "score": 3
                            }
                        ]
                    }
                ]
            },
            {
                "id": "post_6_c5",
                "text": "He's been vetoing all over. I bet he has a big veto stamp.",
                "score": 60,
                "replies": [
                    {
                        "id": "post_6_c5_r1",
                        "text": "itâ€™s been our system for the last 45 years. lawmakers freely support anything thatâ€™s popular. the governor vetos the crazy stuff. nobody ever calls for a veto override. everyoneâ€™s happy.",
                        "score": 26
                    }
                ]
            },
            {
                "id": "post_6_c6",
                "text": "Good it was a dumb fuckin bill that woulda been unenforceable except to fuck over companies trying to work with the opensource community and wouldnt do shit to stop china and actual bad actors.",
                "score": 20
            },
            {
                "id": "post_6_c8",
                "text": "First time I agree with him",
                "score": 27,
                "replies": [
                    {
                        "id": "post_6_c8_r1",
                        "text": "understand the sentiment, to give credit where it is due though, he vetoes a lot of insane legislation\n\n(i.e., anything so toxic that it would damage a presidential run...)",
                        "score": 21,
                        "replies": [
                            {
                                "id": "post_6_c8_r1_r1",
                                "text": "I will not disagree with thatâ€”I am not a proponent of his policies normally but I did feel like more regulation is not the answer",
                                "score": 4
                            }
                        ]
                    }
                ]
            },
            {
                "id": "post_6_c9",
                "text": "Gavin came through.",
                "score": 15
            },
            {
                "id": "post_6_c10",
                "text": "He's an idiot, but this was the right move. Even slick idiots can occasionally be right.",
                "score": 12
            },
            {
                "id": "post_6_c11",
                "text": "Only because democrats want it to be illegal for even smaller models than 405b - so not so good news really: \"The Democrat decided to reject the measure because it applies only to the biggest and most expensive AI models and doesnâ€™t take into account whether they are deployed in high-risk situations, he said in his veto message\"",
                "score": 47,
                "replies": [
                    {
                        "id": "post_6_c11_r1",
                        "text": "That's an oversimplified framing for tech-illiterate WSJ readers. He did because the bill doesn't target specific risks that are tangible and evidence based. Instead its provisions are based on hypothetical scenarios and vibes because the science fiction says so.\n\nFortunately Gavin Newsom listened to the empirical experts.",
                        "score": 79,
                        "replies": [
                            {
                                "id": "post_6_c11_r1_r1",
                                "text": "What risks are \"tangible\" and \"evidence based\" in your mind?",
                                "score": 3,
                                "replies": [
                                    {
                                        "id": "post_6_c11_r1_r1_r1",
                                        "text": "For starters, the risks (as Newsom ponts out in his statement) depend on the application. So if you use a model to make clinical health decisions you would regulate it differently from a free joke generator on mobile app. That makes a lot of sense to me. And if you are a Redditor using a model to create your own chatbot (and don't sell it), you can do whatever you want.\n\nThat to me a much, much, much better approach than trying to regulate the models themselves.",
                                        "score": 20
                                    },
                                    {
                                        "id": "post_6_c11_r1_r1_r2",
                                        "text": "Right now we can't think of one. That's why SB 1047 is incredibly unnecessary. It's important to note that AI safety is still nascent and the science and tech still hasn't matured.",
                                        "score": 12,
                                        "replies": [
                                            {
                                                "id": "post_6_c11_r1_r1_r2_r1",
                                                "text": ">That's why SB 1047 is incredibly unnecessary\n\n. \n\n>It's important to note\n\nBoys, I think AI is trying to break out and manipulate public opinion.",
                                                "score": 4
                                            }
                                        ]
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "id": "post_6_c11_r2",
                        "text": "Maybe the silver lining is that it will buy us some time for Llama 4 or some other future model, but yeah in the long run this veto isn't for the right reasons.",
                        "score": 32,
                        "replies": [
                            {
                                "id": "post_6_c11_r2_r1",
                                "text": "I mean its all idiotic, how does this stupid law or a more agressive one stop actual bad actors from china or anywhere, or out of state lol its a dumb bill",
                                "score": 9,
                                "replies": [
                                    {
                                        "id": "post_6_c11_r2_r1_r1",
                                        "text": "It does not. Bad actors will not care about some weird law specific to US, let alone just a single state. These laws do not increase safety either, since get in the way of open research and small busyness taking advantage of the technology. In fact, they decrease safety, not only because reduce open research, but also because more people will depend on corporations even if they or their busyness would be better off running things locally otherwise. But such law would help established corporations to achieve regulatory capture and reduce number of competitors, thus increasing their profit. There is literally nothing else such law would accomplish.\n\nIn the meantime China's models are getting better and better. For example, Qwen2-VL works better in my tests than new Llama3.2 90B. Molmo 72B based on Qwen2-VL is pretty good too (Molmo 72B based on it seems to be great as well). Qwen2.5 72B is quite comparable in capabilities against Llama 70B, in some tasks even better. If Meta would not be allowed to keep up, then it will be left behind - I imagine Qwen3 will have even better capabilities, so Meta has to release Llama 4 in reasonable amount of time in order to keep the lead.",
                                        "score": 4
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "id": "post_6_c11_r3",
                        "text": "Whatever buys time, is a good thing. Since as these things go. The more it's used the harder it is to put back in the tube. Remember, RSA used to be government controlled and restricted as well. As in you could go to prison for it. Until it became so ubiquitous that they had choice but to give that up.",
                        "score": 18
                    },
                    {
                        "id": "post_6_c11_r4",
                        "text": "\"Only\" yet you quote two points.\n\n\"The Democrat decided to reject the measure because it ... doesnâ€™t take into account whether they are deployed in high-risk situations\"\n\nSounds reasonable to me, I don't plan on giving Qwen the nuclear codes any time soon.",
                        "score": 15
                    },
                    {
                        "id": "post_6_c11_r5",
                        "text": "I was about to be shocked, but this makes more sense.  Still, a win is a win, so props to Newsom for picking the correct option, even if it was for a shitty reason",
                        "score": 5
                    },
                    {
                        "id": "post_6_c11_r6",
                        "text": "That's simply PR from Newsom so he wouldn't be a scape goat for his decision.",
                        "score": 5
                    }
                ]
            }
        ]
    },
    {
        "id": "post_7",
        "url": "https://i.redd.it/vqotdmdak6rd1.jpeg",
        "title": "Wen ðŸ‘ï¸ ðŸ‘ï¸?",
        "upvotes": 572,
        "rank": 7,
        "comments": [
            {
                "id": "post_7_c1",
                "text": "Gerganov updated https://github.com/ggerganov/llama.cpp/issues/8010 eleven hours ago with this:\n\n> My PoV is that adding multimodal support is a great opportunity for new people with good software architecture skills to get involved in the project. The general low to mid level patterns and details needed for the implementation are already available in the codebase - from model conversion, to data loading, backend usage and inference. It would take some high-level understanding of the project architecture in order to implement support for the vision models and extend the API in the correct way.\n\n> We really need more people with this sort of skillset, so at this point I feel it is better to wait and see if somebody will show up and take the opportunity to help out with the project long-term. Otherwise, I'm afraid we won't be able to sustain the quality of the project.\n\nSo better to not hold our collective breath.  I'd love to work on this, but can't justify prioritizing it either, unless my employer starts paying me to do it on company time.",
                "score": 131,
                "replies": [
                    {
                        "id": "post_7_c1_r1",
                        "text": "How many years do we have to wait until an LLM can do it? I'm joking, but not really.",
                        "score": 36
                    },
                    {
                        "id": "post_7_c1_r2",
                        "text": "I'd also love to work on it but I don't have the work time to invest into learning enough about the project to implement it.",
                        "score": 11
                    }
                ]
            },
            {
                "id": "post_7_c2",
                "text": "I have some free time and I might have the skills to implement this. Would it really be this useful? I'm usually only interested in text models, but from the comments it seems that people want this. If there is enough demand, I might give it a shot :)",
                "score": 60,
                "replies": [
                    {
                        "id": "post_7_c2_r1",
                        "text": "There is tremendous demand, and we would love you forever.",
                        "score": 31
                    },
                    {
                        "id": "post_7_c2_r2",
                        "text": "Where would a dev start to learn how all of this work if you dont mind sharing?",
                        "score": 6,
                        "replies": [
                            {
                                "id": "post_7_c2_r2_r1",
                                "text": "I'm not a super specialist. I have 10 years or so of C++ experience, with lots of low level embedded stuff and some pet neural network projects.\n\nBut this would be a huge undertaking for me. I'd probably start with the Karpaty videos, then study OpenAI's CLIP and then study the llama.cpp codebase.",
                                "score": 9,
                                "replies": [
                                    {
                                        "id": "post_7_c2_r2_r1_r1",
                                        "text": "It will be far from trivial. But it does represent an opportunity for someone (maybe you?) to create something that will be of enormous and enduring value to a large and expanding community of users. \n\nI can see something like this as being a career - maker for someone wanting a serious leg up in their CV, or a foot in the door to a valuable opportunity with the right company or startup, or a significant part of building a bridge to seed funding for a founding engineer.",
                                        "score": 3
                                    }
                                ]
                            }
                        ]
                    }
                ]
            },
            {
                "id": "post_7_c3",
                "text": "good news! They're open source and looking forward to your contribution",
                "score": 157,
                "replies": [
                    {
                        "id": "post_7_c3_r1",
                        "text": "I really need to learn, to be honest. The kind of work that they are doing feels like magic to a fintech developer like me, but at the same time I feel bad not contributing myself. \n\nI need to take a few weekends and just stare at some PRs that added other architectures in to understand what and why they are doing it, so I can contribute as well. I feel bad just constantly relying on their hard work.",
                        "score": 50,
                        "replies": [
                            {
                                "id": "post_7_c3_r1_r1",
                                "text": "The authors publish their work as open source so that others may benefit from it. You don't need to feel guilty about not contributing (though definitely do so if you are up to it!).\n\nThe trouble starts when people start asking for free work.",
                                "score": 44
                            },
                            {
                                "id": "post_7_c3_r1_r2",
                                "text": "Maybe someone could fine tune a model specifically on all things llama.cpp/gguf/safetensors/etc and have it help? Or build a vector database with all the relevant docs? Or experiment with Gemini's 2 billion context window to teach it via in-context learning.\n\nI wouldn't even know where to find all the relevant documentation. I'd probably fuck it up by tuning/training it on the wrong stuff. Not that I even know how to do *that* stuff in the first place.",
                                "score": 4
                            }
                        ]
                    },
                    {
                        "id": "post_7_c3_r2",
                        "text": "Not everyone has the skill to contribute, and encouraging such people to do so does not help anyone.",
                        "score": 6
                    },
                    {
                        "id": "post_7_c3_r3",
                        "text": "I am contributing. I make memes to gently push them forward, just a bit of kindhearted hazing to motivate them. \nSeriously though, I appreciate them and the work they do. Iâ€™m not smart enough to even comprehend the challenges they are up against to make all this magic possible.",
                        "score": 26
                    }
                ]
            },
            {
                "id": "post_7_c4",
                "text": "Let's pool some money to pay the llama.cpp devs via crowdsourcing?",
                "score": 11
            },
            {
                "id": "post_7_c5",
                "text": "llamacpp MUST goes deeper finally into multimodal models.\n\nSoon that project will be obsolete if they will not do that as most models will be multimodal only.... soon including audio and video (pixtral can text and pictures for instance ) ...",
                "score": 52,
                "replies": [
                    {
                        "id": "post_7_c5_r1",
                        "text": ">pixtral can text, video and pictures for instance\n\nPixtral only supports images and text. There are open VLMs that support video, like Qwen2-VL, but Pixtral does not.",
                        "score": 14
                    },
                    {
                        "id": "post_7_c5_r2",
                        "text": "I'm a bit worried about llamacpp in general. I git pulled a update recently which caused all models to hang forever on load. Saw that others are having the same problem in github issues. I ended up reverting to a hash from a couple months ago...\n\n\nMaybe the project is already getting hard to manage at the current scope. Maintainers are apparently merging PRs that are breaking the codebase, so ggergonov concern about quality seems very real.",
                        "score": 4
                    }
                ]
            },
            {
                "id": "post_7_c6",
                "text": "For a whole month various requests for Qwen2-VL support for llama.cpp have been created, and it feels as if it is a cry into the void, as if no one wants to implement it.\n\nAlso this type of models does not support 4-bit quantization.\n\nI realize that some people have 24+ GB VRAM, but most people don't, so I think it's important to make quantization support for these models so people can use them on weaker graphics cards.\n\nI know this is not easy to implement, but for example Molmo-7B-D already has BnB 4bit quantization.",
                "score": 22,
                "replies": [
                    {
                        "id": "post_7_c6_r1",
                        "text": ">Also this type of models does not support 4-bit quantization.\n\nThat's not completely accurate. Most VLMs support quantizing. [Qwen2-VL](https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d) has official 4-bit [GPTQ ](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct-GPTQ-Int4)and [AWQ](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct-AWQ) quants.\n\nI imagine Molmo will get similar quants at some point as well.",
                        "score": 11,
                        "replies": [
                            {
                                "id": "post_7_c6_r1_r1",
                                "text": "Unlikely, the AutoAWQ and AutoGPQ packages have very sparse support for vision models as well. The only reason qwen has these models in said format is because they added the PR themselves.",
                                "score": 4
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "id": "post_8",
        "url": "https://huggingface.co/amd/AMD-Llama-135m",
        "title": "AMD Unveils Its First Small Language Model AMD-135M",
        "upvotes": 469,
        "rank": 8,
        "comments": [
            {
                "id": "post_8_c1",
                "text": "AMD, please put your effort into developing and supporting ROCm. Get your developers contributing to the projects that would benefit from using your hardware if ROCm was mature. Make it work, make it easy. I would love to throw my money at you. Get your shit together.",
                "score": 544,
                "replies": [
                    {
                        "id": "post_8_c1_r1",
                        "text": "+1\n\nI can't help but think this is a knee-jerk reaction to somewhat recent Nvidia work with Nemo, etc models.\n\nNvidia is at the point where it makes sense. AMD should recognize that they are years behind Nvidia in terms of software and ecosystem support and focus their energies in that direction.",
                        "score": 96,
                        "replies": [
                            {
                                "id": "post_8_c1_r1_r1",
                                "text": "> they are years behind Nvidia in terms of software and ecosystem support and focus their energies in that direction.\n\nI remember reading discussions on Reddit and hn a decade ago about how AMD was hopelessly behind Cuda and shouldnâ€™t even bother. Then rocm came along and people were hopeful that AMD would really try.  But they half assed it. Had they tried harder they would be in a much better position now even if they never caught up. A decade of iteration is a beautiful thing.",
                                "score": 35
                            },
                            {
                                "id": "post_8_c1_r1_r2",
                                "text": "> I can't help but think\n\nI almost wrote \"I couldn't help but laugh\" at Koksny's comment above.\n\nI think we're being fine tuned by these LLMs",
                                "score": 21,
                                "replies": [
                                    {
                                        "id": "post_8_c1_r1_r2_r1",
                                        "text": "Your comment sent shivers down my spine",
                                        "score": 21,
                                        "replies": [
                                            {
                                                "id": "post_8_c1_r1_r2_r1_r1",
                                                "text": "The LLMs are weaving us into a tapestry of disaster.",
                                                "score": 20,
                                                "replies": [
                                                    {
                                                        "id": "post_8_c1_r1_r2_r1_r1_r1",
                                                        "text": "The tapestry of weaves",
                                                        "score": 6,
                                                        "replies": [
                                                            {
                                                                "id": "post_8_c1_r1_r2_r1_r1_r1_r1",
                                                                "text": "Let us avoid delving.",
                                                                "score": 8
                                                            },
                                                            {
                                                                "id": "post_8_c1_r1_r2_r1_r1_r1_r2",
                                                                "text": "The Wheel weaves as the Wheel will.",
                                                                "score": 6
                                                            }
                                                        ]
                                                    }
                                                ]
                                            }
                                        ]
                                    },
                                    {
                                        "id": "post_8_c1_r1_r2_r2",
                                        "text": "It is us that's being aligned all along. Alien-seeded technology to herd the humans.",
                                        "score": 7
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "id": "post_8_c1_r2",
                        "text": "Best they can do is monthly press announcements about nothing, and threatening to sue the zluda dev.",
                        "score": 18
                    },
                    {
                        "id": "post_8_c1_r3",
                        "text": "It's been a while since AMD updated rocm for windows...",
                        "score": 60,
                        "replies": [
                            {
                                "id": "post_8_c1_r3_r1",
                                "text": "It has been 8 years before they even half-assed an actual Windows release...",
                                "score": 57,
                                "replies": [
                                    {
                                        "id": "post_8_c1_r3_r1_r1",
                                        "text": "so damn disappointing",
                                        "score": 14
                                    },
                                    {
                                        "id": "post_8_c1_r3_r1_r2",
                                        "text": "LOL! (I felt the pain of Vega, but this comment still made me laugh)",
                                        "score": 5,
                                        "replies": [
                                            {
                                                "id": "post_8_c1_r3_r1_r2_r1",
                                                "text": "What are you talking about?\n\nVega Processing was an amazing >!track from Doom 2016!<.",
                                                "score": 3
                                            }
                                        ]
                                    }
                                ]
                            },
                            {
                                "id": "post_8_c1_r3_r2",
                                "text": "It's updated on Linux.",
                                "score": 10,
                                "replies": [
                                    {
                                        "id": "post_8_c1_r3_r2_r1",
                                        "text": "I love linux, but that doesn't mean people on windows should get ROCm updates once in a blue moon.",
                                        "score": 11,
                                        "replies": [
                                            {
                                                "id": "post_8_c1_r3_r2_r1_r1",
                                                "text": "There's very little point developing ROCm for windows when DirectML exists. It makes more sense they'd want to contribute to a more universal standard for consumer use, which they do.",
                                                "score": 3
                                            }
                                        ]
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "id": "post_8_c1_r4",
                        "text": "I'm a big \"get your shit together AMD and make sure ROCm is working on everything\" proponent as well, but this is the type of project that's exactly that?\n\nThe people who trained this (looks like a small, 2 dev project) aren't the same people working on drivers, but what they **did** is write (and Apache 2.0'd) some useful application code for using both ROCm and RyzenAI (for NPU) for both a [multi-node training run](https://github.com/AMD-AIG-AIMA/AMD-LLM/tree/main/cluster) (using PyTorch Lightning) and a [GPU+NPU speculative coding implementation](https://github.com/AMD-AIG-AIMA/AMD-LLM/blob/main/speculative_decoding/codellama_spec.py).\n\nTo act like this doesn't that directly isn't part of \"make it work, make it easy\" is pretty shortsighted. Working examples/implementation code is pretty key to AMD hardware adoption and this will make the lives easier for anyone jumping in and trying to do either training or advanced inference on AMD, so what's the problem?",
                        "score": 29,
                        "replies": [
                            {
                                "id": "post_8_c1_r4_r1",
                                "text": "As a developer, many times where I'm in the unknown territory, especially where documentation is lacking, sample code from solution creator saved me many hours of blind research and try and fails. +1 for this guy ^",
                                "score": 14
                            },
                            {
                                "id": "post_8_c1_r4_r2",
                                "text": "Also, dogfooding is essential when creating a platform. Otherwise, you work on features that your users don't need, or don't notice misbehavior etc.",
                                "score": 4
                            }
                        ]
                    },
                    {
                        "id": "post_8_c1_r5",
                        "text": "So true. ROCm needs more absolute representation in the developer's headspace",
                        "score": 10
                    },
                    {
                        "id": "post_8_c1_r6",
                        "text": "A lot of progress is being made... for MI300s. And apparently NPUs?",
                        "score": 16,
                        "replies": [
                            {
                                "id": "post_8_c1_r6_r1",
                                "text": "Right so nothing normal users care about",
                                "score": 6,
                                "replies": [
                                    {
                                        "id": "post_8_c1_r6_r1_r1",
                                        "text": "Normal users benefit from this as normal users will get AI from integrated products. We are the abnormal users...",
                                        "score": 17,
                                        "replies": [
                                            {
                                                "id": "post_8_c1_r6_r1_r1_r1",
                                                "text": "I prefer the term paranormal user. If you slight us we will haunt your git repo for seven days.",
                                                "score": 9
                                            }
                                        ]
                                    }
                                ]
                            },
                            {
                                "id": "post_8_c1_r6_r2",
                                "text": "And the Radeon 7900 series. In theory ROCm can work on other 7000-series GPUs but officially I think it's still just 7900. \n\nBut I think the message is clear: this is going to be part of their consumer GPUs going forward. It's natural for that to begin at the high end. Everyone knows they're a generation or two behind Nvidia with this stuff, but they're catching up.",
                                "score": 4,
                                "replies": [
                                    {
                                        "id": "post_8_c1_r6_r2_r1",
                                        "text": "It's still partial. For instance, they worked on flash attention for the MI300, but I think it *still* doesn't work on other AMD cards, right?",
                                        "score": 8,
                                        "replies": [
                                            {
                                                "id": "post_8_c1_r6_r2_r1_r1",
                                                "text": "Yeah, that doesn't work on the 7900 last I checked. :(",
                                                "score": 4,
                                                "replies": [
                                                    {
                                                        "id": "post_8_c1_r6_r2_r1_r1_r1",
                                                        "text": "Yeah, I mean thats huge if you want long context. No xformers either, right?\n\nIts kinda like being on mac, where you can get flash attention through llama.cpp, but then you are stuck with it.",
                                                        "score": 3
                                                    }
                                                ]
                                            },
                                            {
                                                "id": "post_8_c1_r6_r2_r1_r2",
                                                "text": "There's an older branch that works, but only accelerates forward attention: https://github.com/ROCm/flash-attention/tree/howiejay/navi_support\n\nThere's also a pure Triton implementation that reportedly works on RDNA3, but I've not tested it yet. And there's also an incomplete implementation using rocWMMA that does support backwards attention, but I've not tested that one either: https://github.com/Repeerc/flash-attention-v2-RDNA3-minimal",
                                                "score": 3
                                            }
                                        ]
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "id": "post_8_c1_r7",
                        "text": "They were going to make it cuda comparable but fired the developer. Ironically you get better performance with a cuda translation layer than with raw ROCm in many tasks.",
                        "score": 10
                    },
                    {
                        "id": "post_8_c1_r8",
                        "text": "AMD is a sponsor of [vLLM](https://github.com/vllm-project/vllm), for what it's worth.",
                        "score": 5
                    },
                    {
                        "id": "post_8_c1_r9",
                        "text": "It's like these companies forget why Microsoft basically gives away Windows and Office to schools. PUSH DEVELOPER $ to open AI, how is this difficult. It's not even going to take some large % of your gross!",
                        "score": 3
                    }
                ]
            },
            {
                "id": "post_8_c2",
                "text": "AMD-Llama-135m is a language model trained on AMD MI250 GPUs. Based on LLaMA2 model architecture, this model can be smoothly loaded as LlamaForCausalLM with huggingface transformers. Furthermore, we use the same tokenizer as LLaMA2, enabling it to be a draft model of speculative decoding for LLaMA2 and CodeLlama.\n\nhttps://community.amd.com/t5/ai/amd-unveils-its-first-small-language-model-amd-135m/ba-p/711368\n\nhttps://www.amd.com/en/developer/resources/technical-articles/introducing-amd-first-slm-135m-model-fuels-ai-advancements.html\n\nhttps://github.com/AMD-AIG-AIMA/AMD-LLM",
                "score": 94,
                "replies": [
                    {
                        "id": "post_8_c2_r1",
                        "text": "1. llama2\n2. Wouldn't it be wrong most of the time, negating the gains of speculative decoding?",
                        "score": 40,
                        "replies": [
                            {
                                "id": "post_8_c2_r1_r1",
                                "text": "It's AMD what did you expect lol. You're lucky they didn't choose unmodified GPT2 arch.",
                                "score": 55
                            },
                            {
                                "id": "post_8_c2_r1_r2",
                                "text": "Llama 1, 2, 3 and 3.1 have the same architecture\n\nEdit: 3.2 not vision too",
                                "score": 9,
                                "replies": [
                                    {
                                        "id": "post_8_c2_r1_r2_r1",
                                        "text": "Llama 2 and 3 have different tokenizers, also llama 3 uses grouped query attention for all model sizes unlike llama 2. As far as I know, llama 2 has it only in the 70b version. I think that's pretty much it. So they are similar but not the same",
                                        "score": 7,
                                        "replies": [
                                            {
                                                "id": "post_8_c2_r1_r2_r1_r1",
                                                "text": "From the code perspective you're calling the same architecture with different numbers in config, the attention depends on the number of heads and kv heads used, llama 1 had the same amount of attention and kv heads which makes multi head attention, llama 2 <70b also have the same number of kv and attention heads, l2 70b and l3 have attention heads divisible by kv heads and kv heads > 1 which makes gqa, if the number of kv heads is 1 and number of attention heads is not then it's multi query attention",
                                                "score": 3
                                            }
                                        ]
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "id": "post_8_c2_r2",
                        "text": "This reads like itâ€™s just an imitation of Andrej Karpathyâ€™s work with his NanoGPT project. Same size and architecture.  He did it by himself (though using some nice fineweb data) on a single A100  box. Him doing it alone is really impressive. Them releasing this isnâ€™t impressive at all.",
                        "score": 22,
                        "replies": [
                            {
                                "id": "post_8_c2_r2_r1",
                                "text": "This uses a different architecture and dataset. I suppose it serves mostly as a demonstration of how you can use AMD gpuâ€™s to train LLMs, in the current NVIDIA-dominated landscape.\n\nThat said, it seems they use litgpt, which is basically a much more built out version of nanogpt. This may serve as a way to pull people in by showing them they can work with a familiar codebase.",
                                "score": 7
                            }
                        ]
                    },
                    {
                        "id": "post_8_c2_r3",
                        "text": "AMD: Hey guys, I have great news, you can now use a predictor model for the state of the art model of llama-2!\n\nhttps://preview.redd.it/n772ou8meird1.jpeg?width=500&format=pjpg&auto=webp&s=1d18cf77c252d69742b7aefeb2fc9456616fd550",
                        "score": 11
                    }
                ]
            },
            {
                "id": "post_8_c3",
                "text": "The training code, dataset and weights for this model are open sourced so that developers can reproduce the model and help train other SLMs and LLMs.\n\nThis is a full actual open source LLM.",
                "score": 162,
                "replies": [
                    {
                        "id": "post_8_c3_r1",
                        "text": "How many datasets are public and available for model training nowadays?",
                        "score": 15,
                        "replies": [
                            {
                                "id": "post_8_c3_r1_r1",
                                "text": "Lots of fine-tuning datasets, but not many comprehensive training datasets.  K2 is the only one that comes to mind.",
                                "score": 25,
                                "replies": [
                                    {
                                        "id": "post_8_c3_r1_r1_r1",
                                        "text": "There's also Dolma [https://allenai.github.io/dolma/](https://allenai.github.io/dolma/) (used by OLMo, similarly open like the LLM360 models) and if you're looking for datasets, FineWeb: [https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1)",
                                        "score": 6
                                    }
                                ]
                            },
                            {
                                "id": "post_8_c3_r1_r2",
                                "text": "A lot. Like it's hard to remember them all.\nSlimPajama, Gutenberg and starcoder they've used is nothing new. Especially Gutenberg.  PG19 was released 5 years ago for anyone lazy enough to make it themselves. SlimPajama is crafted from red pajama whose announcement [gave me a phone wallpaper](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/6530041a65d998238693e003_RedPajama.png)\n\nPile was available for a long time, c4 is still available. RefinedWeb too from falcon. Olmo uses dolmo.\n\nAnd it's a good thing: datasets are much more valuable than models as you can train better models",
                                "score": 4
                            }
                        ]
                    }
                ]
            },
            {
                "id": "post_8_c4",
                "text": "What's strange is that they are actually doing good research... just not releasing it.\nHere is a paper from months ago on memory efficient training enhancements that AMD was the main contributor on, and it clearly says they aren't releasing the code.\nhttps://arxiv.org/html/2406.08334v1\n\nThey REALLY need to step up their game with the open community if they want to compete in this race.",
                "score": 35
            },
            {
                "id": "post_8_c5",
                "text": "Well, on one hand - it's barely coherent. \n\nOn other - it's 80MB, and it kinda, sometimes sort-of works. Sometimes. Most of the time it's just spewing random but coherent sentences, but sometimes it works:\n\n> USER: What is the capitol of France? \n\n> AMD-135M: The capital of France is at Paris, which has been split into two parts in 1789. The city is the second largest, and its architectural beauty is still admired by the international as well as the French. Its architecture is one of the best in Europe. The buildings on the ÃŽle Janvier are of French origin, such as the cathedral, which was completed in 1702, and the Paris. This building has 274 rooms, each with a different story. It is the only French building in the world to be listed as a World Heritage site and is the only one to be built with the help of a Frenchman.\nThe principal city of France, and the most important for it's history is its monuments. The Louvre is the oldest, and the most prestigious. It was built in 1523 by Louis XV, who lived there until he died in 1674.\n\n> USER: Who is Emanuel Macron?\n\n> Emanuel Macron is a French politician, born on May 20, 1969.\nMacron first won the presidency of France in 2007. He was elected as a pro-forma candidate for the French presidency in 2017. Macron is one of the 380 candidates nominated for the French presidency, which is part of a list of 380 candidates to be nominated in the presidential race. Macron is not an official. Macron is a member of the French parliament.\nMacron also holds the office of Vice-President, and is a Member of the French parliament.\nMacron is a member of the French parliament. He is a member of the French parliament.\nMacron has previously held the office of Vice-President since 2017. He was elected as a pro-forma candidate for the French parliament.\n\nFor a model that takes ~150MB (and the 80MB Q4 doesn't seem to be much worse), this is... something? \n\nFar too unreliable though for any actual use case, unfortunately. But good on AMD to try.",
                "score": 47,
                "replies": [
                    {
                        "id": "post_8_c5_r1",
                        "text": "I don't understand, is it an instruction model or a \"plain\" LLM? Because if it's what I think your evaluation is not fair.",
                        "score": 11,
                        "replies": [
                            {
                                "id": "post_8_c5_r1_r1",
                                "text": "There is no instruct fine-tune, but there is code fine-tune. The examples are just from base model though. \n\nConsidering how small the model is, fine-tuning it for instruct probably takes 10 minutes, so...",
                                "score": 15,
                                "replies": [
                                    {
                                        "id": "post_8_c5_r1_r1_r1",
                                        "text": "So I think it might explain why it is sometimes behaving unexpectedly, it should not be always coherent before aligned.\n\nThanks for the info.",
                                        "score": 6,
                                        "replies": [
                                            {
                                                "id": "post_8_c5_r1_r1_r1_r1",
                                                "text": "I think you might be hitting nail on the head, it might be very good model to experiment with fine-tuning.",
                                                "score": 6,
                                                "replies": [
                                                    {
                                                        "id": "post_8_c5_r1_r1_r1_r1_r1",
                                                        "text": "That's what I thought. I am actually pretty excited to try it. Could be also good as a plain auto complete, grammar correction, etc.",
                                                        "score": 3
                                                    }
                                                ]
                                            }
                                        ]
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "id": "post_8_c5_r2",
                        "text": "On one hand, for 80mb, that's impressive.  OTOH, being AMD, it would look horrible for them to use some CUDA based training, they need to use their hardware, and if that's what they can put out using their hardware, it's pretty sad.  I would love to see a competitor to nVidia, but how can that happen when nVidia has the market on CUDA and most AI is built on it?  AMD is leaps and bounds behind.",
                        "score": 4
                    }
                ]
            },
            {
                "id": "post_8_c6",
                "text": "fix rocm then worry about other stuff",
                "score": 54,
                "replies": [
                    {
                        "id": "post_8_c6_r1",
                        "text": "They most likely used ROCm to do this, one of the biggest problem developers have is when they don't use the tools they developed and then are surprised their tools are shit. The fact they are using their own tools means they are learning their limitations.",
                        "score": 30,
                        "replies": [
                            {
                                "id": "post_8_c6_r1_r1",
                                "text": "This. A lot of times it felt like the ROCm team was out of touch for what their software was really used for. Them creating their own model is kind of exciting because it will force them to work on the limits ROCm gave them during development of the model.",
                                "score": 15
                            }
                        ]
                    }
                ]
            },
            {
                "id": "post_8_c7",
                "text": "Very impressive considering the size of the model and the little it takes to run, people shitting on it apparently didn't understand it enough.",
                "score": 28,
                "replies": [
                    {
                        "id": "post_8_c7_r1",
                        "text": "> people shitting on it apparently didn't understand it enough.\n\nMore or less the definition of Reddit, smart *sounding* (and in many cases actually smart) people knee-jerk-reacting to shit they took approximately zero seconds to try to understand before opinionating loudly and authoritatively.",
                        "score": 20
                    },
                    {
                        "id": "post_8_c7_r2",
                        "text": "Agreed.\n\nBetween its lower vocabulary size and shorter context, the per-parameter memory requirements to train this model are about 5% that of llama3, which means it can be efficiently trained on modest-sized GPUs with large batch sizes.\n\nThat's lost on people, of course.  Most only know AMD from NVIDIA from gamer tribalism, and lack mental compartmentalization skills.",
                        "score": 10
                    }
                ]
            },
            {
                "id": "post_8_c8",
                "text": "What possible use cases exist for a 135M parameter model built on Llama 2? Anyone? No?",
                "score": 27,
                "replies": [
                    {
                        "id": "post_8_c8_r1",
                        "text": "None, really, but as a research toy - it's neat to see inference of almost 1TB datasets from model compressed to 100MB.\n\nAnd we really need development of those very small, edge models, if we want to actually implement language models into day-to-day stuff.",
                        "score": 37
                    },
                    {
                        "id": "post_8_c8_r2",
                        "text": "I assume its a proof of concept, ostensibly for speculative decoding as they say.\n\nI *hope* no one at AMD thinks it would be a model they expect people to use.",
                        "score": 22,
                        "replies": [
                            {
                                "id": "post_8_c8_r2_r1",
                                "text": "I expect AMD thinks their documented training process is something people will use, not the demonstration model.",
                                "score": 8,
                                "replies": [
                                    {
                                        "id": "post_8_c8_r2_r1_r1",
                                        "text": "I'll use the demo model in a game where I need a madman to rant..., seems pretty good in that very specific usecase",
                                        "score": 6
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "id": "post_8_c8_r3",
                        "text": "Next word prediction for mobile keyboard? Really fast and basic sentiment/subject categorizer.",
                        "score": 16
                    },
                    {
                        "id": "post_8_c8_r4",
                        "text": "They have documented their training process, so now anyone has a ready-to-go recipe for training models on AMD+ROCm.\n\n135M is sufficient for a functional demonstration.",
                        "score": 9
                    },
                    {
                        "id": "post_8_c8_r5",
                        "text": "Per the blog post/repo, they also implemented speculative decoding and it apparently works well enough for speeding up CodeLlama and could be used on their NPU as well.",
                        "score": 5,
                        "replies": [
                            {
                                "id": "post_8_c8_r5_r1",
                                "text": "Thing is... codellama was never very good lol, and is definitely not a good choice right now.\n\nThat's absolutely fine as a research toy, but I hope no one at AMD things codellama is a popular end-user thing now.",
                                "score": 5
                            }
                        ]
                    }
                ]
            },
            {
                "id": "post_8_c9",
                "text": "Since everything about this is open source, this can be viewed as a full example of how to use AMD MI cards for LLM training.",
                "score": 12
            },
            {
                "id": "post_8_c10",
                "text": "All the negativity aside I think this is still a welcome development. Hopefully theyâ€™ll invest more in the LLM space. Having more open and free options is never bad.",
                "score": 12
            },
            {
                "id": "post_8_c12",
                "text": "Does it run better on Nvidia GPU ? :PPPP",
                "score": 10
            }
        ]
    },
    {
        "id": "post_9",
        "url": "https://i.redd.it/o8posp1psdrd1.png",
        "title": "NVIDIA Jetson AGX Thor will have 128GB of VRAM in 2025!",
        "upvotes": 454,
        "rank": 9,
        "comments": [
            {
                "id": "post_9_c1",
                "text": "no idea what that AGX thing is, the main question would be, just how many thousands of dollars?",
                "score": 123,
                "replies": [
                    {
                        "id": "post_9_c1_r1",
                        "text": "previous generations: AGX Xavier $1000, AGX Orin $2000\n\nSo... AGX Thor $4000? ðŸ™„",
                        "score": 51,
                        "replies": [
                            {
                                "id": "post_9_c1_r1_r1",
                                "text": "The new scaling law â˜¹ï¸",
                                "score": 57,
                                "replies": [
                                    {
                                        "id": "post_9_c1_r1_r1_r1",
                                        "text": "It will scale until Nvidia lose market in China and then we'll see chip resale in AliExpress. (If it doesn't I'll stick to 3090s ðŸ¤·â€â™€ï¸)",
                                        "score": 21
                                    }
                                ]
                            },
                            {
                                "id": "post_9_c1_r1_r2",
                                "text": "Isnâ€™t that a damn good price though for 128GB of VRAM?",
                                "score": 10
                            }
                        ]
                    },
                    {
                        "id": "post_9_c1_r2",
                        "text": "About 3 Honda Civics",
                        "score": 103,
                        "replies": [
                            {
                                "id": "post_9_c1_r2_r1",
                                "text": "New or used?",
                                "score": 30,
                                "replies": [
                                    {
                                        "id": "post_9_c1_r2_r1_r1",
                                        "text": "New",
                                        "score": 15,
                                        "replies": [
                                            {
                                                "id": "post_9_c1_r2_r1_r1_r1",
                                                "text": "Painted in 24g gold",
                                                "score": 4
                                            }
                                        ]
                                    },
                                    {
                                        "id": "post_9_c1_r2_r1_r2",
                                        "text": "Concept",
                                        "score": 13
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "id": "post_9_c1_r3",
                        "text": "Yes",
                        "score": 5
                    },
                    {
                        "id": "post_9_c1_r4",
                        "text": "All of them.",
                        "score": 5
                    }
                ]
            },
            {
                "id": "post_9_c2",
                "text": "It has **shared memory** same as Macbooks, so you can run LLMs and train small models, it also has ARM64 CPUs, the previous version had 12 Cores so this will have \\~ 20-30 cores.\n\nCompared to Macs you have CUDA, so you can run and play with practically every open source model.\n\nInference wise, it probably is similar speed to Macs, so main selling point is CUDA and Ubuntu.\n\nAlso, you **can't run games**, I tried ðŸ˜… The Box86/64 & Proton needs 32 bit libs.\n\nPricewise expect **2-3k $** but also **50-100W** (It's aimed at Edge & Robotics) so you won't have a sauna in your room.",
                "score": 148,
                "replies": [
                    {
                        "id": "post_9_c2_r1",
                        "text": "Wait that's insanely high vram and power efficiency at such a low cost. Are you sure the numbers are right??",
                        "score": 63,
                        "replies": [
                            {
                                "id": "post_9_c2_r1_r1",
                                "text": "IIRC current Orin is 64gb (200gb/s) at 60w",
                                "score": 37,
                                "replies": [
                                    {
                                        "id": "post_9_c2_r1_r1_r1",
                                        "text": "Ahh ok. I did some digging, and it sounds like Orin is not fast at all in terms of running llm inference (I'm reading numbers like 1 to 4 tokens/sec for Llama2 70B). I dunno who to ask this to, but do you expect Thor to be slow compared to other GPUs in the market in similar price ranges? (Rtx 4090 for example)\n\nEDIT: here's where I got those numbers from - https://www.reddit.com/r/LocalLLaMA/s/uClI0LsDgq",
                                        "score": 24,
                                        "replies": [
                                            {
                                                "id": "post_9_c2_r1_r1_r1_r1",
                                                "text": "The slide posted here says 8x GPU performance of Orin for Transformers. Exciting if true.",
                                                "score": 16,
                                                "replies": [
                                                    {
                                                        "id": "post_9_c2_r1_r1_r1_r1_r1",
                                                        "text": "And 10x IO bandwidth. Hopefully RAM is part of that IO",
                                                        "score": 9,
                                                        "replies": [
                                                            {
                                                                "id": "post_9_c2_r1_r1_r1_r1_r1_r1",
                                                                "text": "Unfortunately is not, but let's hope they put 8 channels on this bad boy at least, which by the look of it, seems it's four channels, but embedded into the system, so maybe is very fast",
                                                                "score": 4
                                                            }
                                                        ]
                                                    }
                                                ]
                                            },
                                            {
                                                "id": "post_9_c2_r1_r1_r1_r2",
                                                "text": "You can look by spec.\nAgx Orin has 200gb/s ram, a 4090 has 1000gb/s ram (same as 3090 BTW).\nI think the test you read was for llama2 70b 8bit isn't it? Using transformer? I think it would be about 4 or 5 times faster on a 3090, so pretty linear to ram speed.\nTht's only for inference ofc\nSo the question is what the agx Thor ram bandwidth?",
                                                "score": 15
                                            },
                                            {
                                                "id": "post_9_c2_r1_r1_r1_r3",
                                                "text": "https://www.jetson-ai-lab.com/benchmarks.html\n\n5 tokens/sec on Llama-70B with MLC/TVM and INT4 quantization.  It's an embedded system for deploying edge inference, so yea it's slower than dGPU.  People mostly using smaller LLM/SLM or VLM/VLA on it optimized for realtime use onboard robots, vision systems, agent kiosks, ect.",
                                                "score": 4,
                                                "replies": [
                                                    {
                                                        "id": "post_9_c2_r1_r1_r1_r3_r1",
                                                        "text": "Oof, that's pretty slow for a 4 bit quantized model. Thanks for sharing. I still have my hopes up for Thor, as it looks like a massive upgrade from Orin",
                                                        "score": 4,
                                                        "replies": [
                                                            {
                                                                "id": "post_9_c2_r1_r1_r1_r3_r1_r1",
                                                                "text": "the thing is that 70B is not small, so the AGX Orin was not meant to run models of that size; if this new one has 128GB, I bet they'd put even faster memory. Capacity and speed have to scale hand-by-hand",
                                                                "score": 3
                                                            }
                                                        ]
                                                    }
                                                ]
                                            }
                                        ]
                                    }
                                ]
                            },
                            {
                                "id": "post_9_c2_r1_r2",
                                "text": "Quick quick everyone sell your 3090s and 4090s now before the price plummets! **hand rubbing noises**",
                                "score": 20
                            }
                        ]
                    },
                    {
                        "id": "post_9_c2_r2",
                        "text": "You got an early access to it? Are you a journalist?",
                        "score": 15
                    },
                    {
                        "id": "post_9_c2_r3",
                        "text": "What are speeds on ie llama 70B?",
                        "score": 7,
                        "replies": [
                            {
                                "id": "post_9_c2_r3_r1",
                                "text": "Current Gen? Slow\nRam is 200gb/s",
                                "score": 5
                            }
                        ]
                    },
                    {
                        "id": "post_9_c2_r4",
                        "text": "Shared memory LPDDR will have fairly disappointing performance, especially for LLMs.\n\nLLMs are highly memory bandwidth constrained and a shared memory LPDDR system will have very poor performance compared to dedicated HBM that you see in proper accelerator devices.",
                        "score": 6
                    },
                    {
                        "id": "post_9_c2_r5",
                        "text": "What VRAM bandwidth? The 10x IO makes me think PCI-E or something else, for the number is too high.",
                        "score": 3
                    },
                    {
                        "id": "post_9_c2_r6",
                        "text": "> Pricewise expect 2-3k $\n\nWhat? How? The current AGX is like $5k. Even the shitty 16GB NX is like $1k.\n\nNvidia never drops their prices, ever.",
                        "score": 4
                    }
                ]
            },
            {
                "id": "post_9_c3",
                "text": "A lot of memory is nice and all. But it is gimped, when it comes to CUDA. AGX have 2048 cores (close to RTX 2060) and a 4090 have 16384. That's 8 times more for the 4090...",
                "score": 42,
                "replies": [
                    {
                        "id": "post_9_c3_r1",
                        "text": "Haha you edited comment ðŸ˜‚ Tech specs are here: [https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/)  \nSo what? It will have **8x less power draw**, and **4x more memory**... (than rumored 5090)\n\nIt's not for training hence less cores...  \nAlso NVIDIA release some \"free\" apps only for x86, i.e. Isaac Sim...",
                        "score": 21
                    },
                    {
                        "id": "post_9_c3_r2",
                        "text": "If you want to run inference you need fast ram, not lots of compute.",
                        "score": 10,
                        "replies": [
                            {
                                "id": "post_9_c3_r2_r1",
                                "text": "It could still impact prompt processing speed, as that is more core dependent than token generation.  So if you're crunching 128k of context frequently there might be a performance loss, but the performance per watt is still rather impressive.",
                                "score": 13,
                                "replies": [
                                    {
                                        "id": "post_9_c3_r2_r1_r1",
                                        "text": "That's an interesting point! Have you made any experiment regarding prompt processing vs compute?",
                                        "score": 4,
                                        "replies": [
                                            {
                                                "id": "post_9_c3_r2_r1_r1_r1",
                                                "text": "You could say I have in the form of my P40s which have quite limited compute compared to their VRAM bandwidth and size.  I can tell you that I have to make heavy use of context shifting in KoboldCPP to use large context sizes.  Vectorization is right out for multi-turn conversations.\n\nSomeone recently posted a chart of different setups and their prompt processing capability versus generation, a 3090 is something like 10x more powerful in prompt processing.",
                                                "score": 5
                                            }
                                        ]
                                    }
                                ]
                            },
                            {
                                "id": "post_9_c3_r2_r2",
                                "text": "I'm totally happy to schedule tasks in an agentic fashion to happen overnight. This would be truly amazing for me if the price point was low.",
                                "score": 3
                            },
                            {
                                "id": "post_9_c3_r2_r3",
                                "text": "Jetson doesn't have fast ram. It is only 204.8 GB/s.",
                                "score": 3
                            }
                        ]
                    }
                ]
            },
            {
                "id": "post_9_c4",
                "text": "I'm sure this will be very affordable and aimed at hobbyists.",
                "score": 24,
                "replies": [
                    {
                        "id": "post_9_c4_r1",
                        "text": "To be fair, it might be the most affordable hardware a hobbyist can buy given the VRAM capacity, compared to other options on the market without turning your setup into a heatpump. Just for comparison, you will need 6 RTX3090 to match 1 AGX Thor, with 20x less power consumption. In my inference book, that's a big win. Let's just hope the price tag is also compatible.",
                        "score": 7
                    }
                ]
            },
            {
                "id": "post_9_c5",
                "text": "This is how the 8GB smol brother looks like: [https://youtu.be/FX2exKW\\_20E?t=8](https://youtu.be/FX2exKW_20E?t=8)  \nIt's mainly used for **robotics**. But with 128GBs you will have powerful **local** machine that has driver support and **vibrant** ecosystem. It's shame that Intel and AMD don't do anything similar ðŸ¥² (competition is always good)",
                "score": 26,
                "replies": [
                    {
                        "id": "post_9_c5_r1",
                        "text": "AMD are doing Halo but it's x86 so it won't be as power efficient.",
                        "score": 3
                    }
                ]
            },
            {
                "id": "post_9_c6",
                "text": "Its a bit disingenuous to call it VRAM. I doubt it will use GDDR6/7, so its much closer to regular LPDDR5x RAM. Sounds like around 400GB/sec vs the 5090 which will have 1,700GB/sec so that's a massive difference. If you like running big models slow then this is for you I guess, but it would probably be cheaper to get a server processor and a bunch of DDR4 sticks.",
                "score": 10
            }
        ]
    },
    {
        "id": "post_10",
        "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ftjbz3/shockingly_good_superintelligent_summarization/",
        "title": "Shockingly good super-intelligent summarization prompt",
        "upvotes": 431,
        "rank": 10,
        "comments": [
            {
                "id": "post_10_c1",
                "text": "I like this, it looks good. The only thing I would add is to clarify what you mean by \"in detail\", as this can vary a lot depending on the model. For example you could do:\n\n\n\"Answer each question in 4-5 sentences. Include a specific example to illustrate your point.\"",
                "score": 48,
                "replies": [
                    {
                        "id": "post_10_c1_r1",
                        "text": "I tried a few much more complex prompts but the model failed to follow the instructions. This was the first one which produced an excellent result.",
                        "score": 28
                    }
                ]
            },
            {
                "id": "post_10_c2",
                "text": "Is the 5 synthetic question/answer pairs always enough to capture the context of longer documents or do you scale the question/answer pairs based on the total length?",
                "score": 14,
                "replies": [
                    {
                        "id": "post_10_c2_r1",
                        "text": "I use 100 000 letters chunks. It is excellent that way (for me).",
                        "score": 11
                    }
                ]
            },
            {
                "id": "post_10_c3",
                "text": "Wait is generating question & answer pairs not widely known? If not, I recommend looking at all the foundational NLP tasks and then mix them together..   \n  \nSuch as create a question answer pair and provide 10 different categorizations. Or list all the topics discussed in the order of how prominent they are in the text. Tell me what the article isn't saying but is implying.. etc etc.",
                "score": 30,
                "replies": [
                    {
                        "id": "post_10_c3_r1",
                        "text": "I thought so too, but I have never read anything about it here. Thats where my post came from",
                        "score": 9,
                        "replies": [
                            {
                                "id": "post_10_c3_r1_r1",
                                "text": "Thank you for your post!!!",
                                "score": 5
                            }
                        ]
                    },
                    {
                        "id": "post_10_c3_r2",
                        "text": "For me the shocking part was making a way better summary by creating synthetic questions from the input. It was like a 30 point IQ level jump for the LLM.",
                        "score": 8,
                        "replies": [
                            {
                                "id": "post_10_c3_r2_r1",
                                "text": "Can you share your examples and elaborate?",
                                "score": 5,
                                "replies": [
                                    {
                                        "id": "post_10_c3_r2_r1_r1",
                                        "text": "I used almost all smaller models capable of summarizing at least 32k tokens of texts. Mistral Small and Qwen 2.5 were the best. I used texts which I wrote to test the level of understanding (this is the best method). But almost all summarizations were superficial. They just didn't \"get\" what I meant. This prompt \"gets\" what I meant when I wrote the texts.",
                                        "score": 5
                                    }
                                ]
                            },
                            {
                                "id": "post_10_c3_r2_r2",
                                "text": "It's a transformer model it will get smarter when you load up the context.Â \n\n\nStart with broad topics at the top and then narrow down as you get lower. The broad and mid range information will help inform what comes after.Â \n\n\nThere is lots of ways to do this from a prompting strategy. Multishot in context learning.. doesn't really matter what the approach is as long as you have everything in text.. it can only calculate on tokens so it must be written or you won't get the benefit.Â ",
                                "score": 4
                            },
                            {
                                "id": "post_10_c3_r2_r3",
                                "text": "What about doing it in 3 steps? First prompt it to write the questions, then prompt it to answer them, and finally prompt it to write a summary based on the answers? Most prompt chaining research has shown that it's better to only give one task per prompt and just keep the \"conversation\" going back and forth, rather than expecting the model to do multiple reasoning steps in a single generation (O1 is the exception obvs because behind the scenes it IS a chain)",
                                "score": 3
                            }
                        ]
                    }
                ]
            },
            {
                "id": "post_10_c6",
                "text": "Localllama discovers prompt engineering. \n\n\nJust jesting of course but wait until you discover libraries like dspy which find out the best prompt for a given measurable use case itself ;)",
                "score": 13,
                "replies": [
                    {
                        "id": "post_10_c6_r1",
                        "text": "You mean this? [GitHub - stanfordnlp/dspy: DSPy: The framework for programmingâ€”not promptingâ€”foundation models](https://github.com/stanfordnlp/dspy?tab=readme-ov-file)",
                        "score": 7,
                        "replies": [
                            {
                                "id": "post_10_c6_r1_r1",
                                "text": "yes. it's basically a meta prompter.\n\nyou basically say what should come out, and how you can measure the quality of the output. than it will optimize prompts, meta parameters like x-shot examples, how the cot thread should look like, basically every parameter you want, even the model itself, until you have a system with max output quality. \n\nThis is basically your idea in DSPY form:\n\nhttps://imgur.com/a/uWbkGC4\n\nIt takes the context, comes up with some questions that if answered improve result, answers those question, creates the improved result, and then creates new questions.\n\nYou may ask: Where are the prompts? And how do the exact prompts look like? who the fuck knows, that's the point. You let it optimize until you have no money anymore, and you have a systemt that's WAAY better than any human designed prompt.",
                                "score": 4
                            }
                        ]
                    }
                ]
            }
        ]
    }
]